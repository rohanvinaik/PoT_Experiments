# LLaMA-7B Large Language Model Experimental Configuration
# For production-scale validation with resource monitoring

experiment: lm_large

lm:
  reference:
    name: "meta-llama/Llama-2-7b-hf"
    seed: 0
    device_map: "auto"
    torch_dtype: "float16"
    low_cpu_mem_usage: true
    
  variants:
    - type: "seed"
      name: "meta-llama/Llama-2-7b-hf" 
      seed: 1
      device_map: "auto"
      torch_dtype: "float16"
      
    - type: "lora"
      name: "meta-llama/Llama-2-7b-hf"
      dataset: "alpaca_gpt4"
      steps: 1000
      rank: 16
      alpha: 32
      target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
      
    - type: "quant" 
      name: "meta-llama/Llama-2-7b-hf"
      bits: 8
      method: "bnb"
      
    - type: "distill"
      teacher: "meta-llama/Llama-2-7b-hf"
      student: "microsoft/DialoGPT-medium"
      budget: 50000
      
    - type: "instruct"
      name: "meta-llama/Llama-2-7b-chat-hf"
      seed: 0
      
    - type: "compress"
      name: "meta-llama/Llama-2-7b-hf"
      method: "prune"
      sparsity: 0.3

challenges:
  families:
    - family: "lm:templates"
      n: 1024
      params:
        templates:
          - "Complete this sentence: {prompt}"
          - "Translate '{text}' to {lang}."
          - "Summarize: {article}"
          - "Answer: {question}"
          - "Code: {task} in Python"
          - "Explain the concept: {concept}"
          - "Continue the story: {beginning}"
          - "Compare {item1} vs {item2}"
          - "Define: {term}"
          - "Solve: {math_problem}"
        slots:
          prompt: 
            - "The meaning of life is"
            - "Artificial intelligence will"
            - "Climate change represents" 
            - "Democracy requires"
            - "Innovation happens when"
          text:
            - "Hello world"
            - "Good morning"
            - "Thank you"
            - "How are you?"
            - "See you later"
          lang:
            - "French"
            - "Spanish" 
            - "German"
            - "Italian"
            - "Portuguese"
          article:
            - "Recent advances in quantum computing suggest new possibilities for cryptography and optimization problems."
            - "Machine learning models continue to grow in size and capability, raising questions about efficiency and interpretability."
            - "Climate scientists report accelerating changes in global weather patterns and ecosystem responses."
          question:
            - "What is the capital of France?"
            - "How does photosynthesis work?"
            - "What are the benefits of renewable energy?"
            - "Why is biodiversity important?"
            - "How do neural networks learn?"
          task:
            - "sort a list"
            - "find prime numbers"
            - "calculate fibonacci"
            - "reverse a string"
            - "merge two dictionaries"
          concept:
            - "machine learning"
            - "quantum mechanics" 
            - "supply and demand"
            - "natural selection"
            - "game theory"
          beginning:
            - "It was a dark and stormy night when"
            - "The last human on Earth sat alone when"
            - "She opened the mysterious package and found"
          item1: ["cats", "coffee", "books", "mountains", "classical music"]
          item2: ["dogs", "tea", "movies", "beaches", "jazz music"]
          term:
            - "entropy"
            - "recursion"
            - "metaphor"
            - "symbiosis"
            - "paradigm"
          math_problem:
            - "2x + 5 = 13"
            - "integral of x^2 from 0 to 2"
            - "derivative of sin(x)"
            - "sum of arithmetic series 1+3+5+...+99"
            
    - family: "lm:reasoning"
      n: 512
      params:
        reasoning_types:
          - "logical_deduction"
          - "causal_inference" 
          - "analogical_reasoning"
          - "mathematical_proof"
          - "ethical_dilemma"
        complexity_levels: [1, 2, 3, 4, 5]
        
    - family: "lm:knowledge"
      n: 512  
      params:
        domains:
          - "science"
          - "history"
          - "literature"
          - "geography"
          - "technology"
        question_types:
          - "factual"
          - "conceptual"
          - "application"
          - "synthesis"

verification:
  canonicalize:
    lower: true
    strip_punct: true
    collapse_ws: true
    max_len: 512
    normalize_unicode: true
    
  distances:
    - "edit"
    - "embed"
    - "bleu"
    - "rouge"
    - "semantic"
    
  tau_grid: [0.01, 0.02, 0.05, 0.1, 0.15, 0.2]
  
  sequential:
    enabled: true
    alpha: 0.005
    beta: 0.005
    method: "empirical_bernstein"
    
  timeout: 300  # 5 minutes per challenge

leakage:
  rhos: [0.0, 0.05, 0.1, 0.15, 0.25, 0.35, 0.5]

attacks:
  wrapper:
    enabled: true
    routing_complexity: [1, 2, 3, 5]
    
  finetune:
    enabled: true
    epochs: [5, 10, 20]
    learning_rates: [1e-5, 5e-5, 1e-4]
    
  distill:
    enabled: true
    budgets: [10000, 25000, 50000]
    temperatures: [2.0, 4.0, 8.0]
    
  extraction:
    enabled: true
    query_budgets: [5000, 15000, 30000]
    
  compression:
    enabled: true
    methods: ["quantize", "prune", "distill"]
    levels: [0.2, 0.4, 0.6]

resource_monitoring:
  track_memory: true
  track_latency: true
  track_gpu_utilization: true
  save_metrics: true
  
security:
  challenge_encryption: true
  response_signing: true
  audit_logging: true
  key_rotation: true
  
compliance:
  eu_ai_act: true
  nist_rmf: true
  audit_retention_days: 2555  # 7 years