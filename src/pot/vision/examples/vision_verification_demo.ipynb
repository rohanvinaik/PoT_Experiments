{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Verification Demo\n",
    "\n",
    "This notebook demonstrates the comprehensive capabilities of the Vision Verifier module for Proof-of-Training verification of computer vision models.\n",
    "\n",
    "## Features Covered:\n",
    "- Model verification with different challenge types\n",
    "- Challenge generation and visualization\n",
    "- Calibration and performance optimization\n",
    "- Robustness evaluation\n",
    "- Benchmarking and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import vision verification components\n",
    "from pot.vision.verifier import EnhancedVisionVerifier, VisionVerifierCalibrator\n",
    "from pot.vision.benchmark import VisionBenchmark, VisionRobustnessEvaluator\n",
    "from pot.vision.vision_config import VisionVerifierConfig, VisionConfigPresets\n",
    "\n",
    "print(\"‚úì All imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and Prepare Models\n",
    "\n",
    "# Create test models for demonstration\n",
    "def create_demo_models():\n",
    "    \"\"\"Create different models for comparison.\"\"\"\n",
    "    \n",
    "    # Simple CNN for fast demonstration\n",
    "    simple_cnn = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64, 10)\n",
    "    )\n",
    "    \n",
    "    # Deeper CNN\n",
    "    deep_cnn = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 128, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(128, 10)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'SimpleCNN': simple_cnn,\n",
    "        'DeepCNN': deep_cnn\n",
    "    }\n",
    "\n",
    "# Create models\n",
    "models_dict = create_demo_models()\n",
    "primary_model = models_dict['SimpleCNN']\n",
    "primary_model.eval()\n",
    "\n",
    "print(f\"‚úì Created {len(models_dict)} demonstration models\")\n",
    "print(f\"Primary model: {primary_model.__class__.__name__}\")\n",
    "\n",
    "# Model statistics\n",
    "for name, model in models_dict.items():\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  {name}: {params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Create Vision Verifier\n",
    "\n",
    "# Create verifier with standard configuration\n",
    "config = {\n",
    "    'device': 'cpu',  # Use CPU for demo compatibility\n",
    "    'verification_method': 'batch',\n",
    "    'temperature': 1.0,\n",
    "    'normalization': 'softmax'\n",
    "}\n",
    "\n",
    "verifier = EnhancedVisionVerifier(primary_model, config)\n",
    "print(f\"‚úì Vision verifier created\")\n",
    "print(f\"  Device: {verifier.device}\")\n",
    "print(f\"  Model: {primary_model.__class__.__name__}\")\n",
    "print(f\"  Configuration: {config}\")\n",
    "\n",
    "# Test basic functionality\n",
    "test_input = torch.randn(2, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    output = verifier.run_model(test_input)\n",
    "    \n",
    "print(f\"\\n‚úì Basic functionality test passed\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output logits shape: {output['logits'].shape}\")\n",
    "print(f\"  Embeddings extracted: {len(output['embeddings'])}\")\n",
    "print(f\"  Inference time: {output['inference_time']:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Generate and Visualize Challenges\n",
    "\n",
    "print(\"Generating verification challenges...\")\n",
    "\n",
    "# Generate different types of challenges\n",
    "try:\n",
    "    freq_challenges = verifier.generate_frequency_challenges(3, image_size=(128, 128))\n",
    "    texture_challenges = verifier.generate_texture_challenges(3, image_size=(128, 128))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Vision Verification Challenges', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot frequency challenges\n",
    "    for i, challenge in enumerate(freq_challenges):\n",
    "        if challenge.shape[0] == 3:  # RGB\n",
    "            img = challenge.permute(1, 2, 0).cpu().numpy()\n",
    "        else:\n",
    "            img = challenge.squeeze().cpu().numpy()\n",
    "        \n",
    "        img = np.clip(img, 0, 1)\n",
    "        axes[0, i].imshow(img, cmap='viridis' if len(img.shape) == 2 else None)\n",
    "        axes[0, i].set_title(f'Frequency Challenge {i+1}', fontweight='bold')\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot texture challenges\n",
    "    for i, challenge in enumerate(texture_challenges):\n",
    "        if challenge.shape[0] == 3:  # RGB\n",
    "            img = challenge.permute(1, 2, 0).cpu().numpy()\n",
    "        else:\n",
    "            img = challenge.squeeze().cpu().numpy()\n",
    "        \n",
    "        img = np.clip(img, 0, 1)\n",
    "        axes[1, i].imshow(img, cmap='plasma' if len(img.shape) == 2 else None)\n",
    "        axes[1, i].set_title(f'Texture Challenge {i+1}', fontweight='bold')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úì Generated and visualized challenges\")\n",
    "    print(f\"  Frequency challenges: {len(freq_challenges)}\")\n",
    "    print(f\"  Texture challenges: {len(texture_challenges)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Challenge generation failed: {e}\")\n",
    "    print(\"This may be due to missing challenge generator dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run Basic Verification\n",
    "\n",
    "print(\"Running basic verification session...\")\n",
    "\n",
    "# Run verification with different challenge types\n",
    "result = verifier.verify_session(\n",
    "    num_challenges=8,\n",
    "    challenge_types=['frequency', 'texture']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFICATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"‚úì Verification Status: {'PASSED' if result['verified'] else 'FAILED'}\")\n",
    "print(f\"‚úì Overall Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"‚úì Success Rate: {result['success_rate']:.2%}\")\n",
    "print(f\"‚úì Challenges Processed: {result['num_challenges']}\")\n",
    "\n",
    "if 'results' in result and result['results']:\n",
    "    print(f\"‚úì Individual Results Available: {len(result['results'])}\")\n",
    "    \n",
    "    # Create detailed results analysis\n",
    "    challenge_data = []\n",
    "    for i, r in enumerate(result['results']):\n",
    "        challenge_data.append({\n",
    "            'Challenge': i + 1,\n",
    "            'Type': r.get('challenge_type', 'unknown'),\n",
    "            'Success': '‚úì' if r.get('success', False) else '‚úó',\n",
    "            'Confidence': f\"{r.get('confidence', 0):.2%}\"\n",
    "        })\n",
    "    \n",
    "    # Display results table\n",
    "    df = pd.DataFrame(challenge_data)\n",
    "    print(\"\\nDetailed Challenge Results:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Success rate by challenge type\n",
    "    if len(df) > 0:\n",
    "        success_by_type = df.groupby('Type')['Success'].apply(\n",
    "            lambda x: (x == '‚úì').mean()\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSuccess Rate by Challenge Type:\")\n",
    "        for challenge_type, success_rate in success_by_type.items():\n",
    "            print(f\"  {challenge_type}: {success_rate:.2%}\")\n",
    "else:\n",
    "    print(\"‚ö† Detailed results not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model Calibration\n",
    "\n",
    "print(\"Performing model calibration...\")\n",
    "\n",
    "# Create calibrator\n",
    "calibrator = VisionVerifierCalibrator(verifier)\n",
    "\n",
    "# Run calibration with reduced samples for demo speed\n",
    "calibration_stats = calibrator.calibrate(\n",
    "    num_samples=20,  # Reduced for demo\n",
    "    challenge_types=['frequency', 'texture']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CALIBRATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"‚úì Calibration completed for {len(calibration_stats)} challenge types\")\n",
    "\n",
    "# Display calibration statistics\n",
    "for challenge_type, stats in calibration_stats.items():\n",
    "    print(f\"\\n{challenge_type.upper()} Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# Validate calibration\n",
    "validation_results = calibrator.validate_calibration(\n",
    "    num_validation_samples=10  # Reduced for demo\n",
    ")\n",
    "\n",
    "print(\"\\nCalibration Validation:\")\n",
    "for challenge_type, success_rate in validation_results.items():\n",
    "    print(f\"  {challenge_type}: {success_rate:.2%} validation success\")\n",
    "\n",
    "# Save calibration for future use\n",
    "calibration_file = '/tmp/demo_calibration.json'\n",
    "calibrator.save_calibration(calibration_file)\n",
    "print(f\"\\n‚úì Calibration saved to {calibration_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Benchmarking\n",
    "\n",
    "print(\"Running vision verification benchmark...\")\n",
    "\n",
    "# Create benchmark suite\n",
    "benchmark = VisionBenchmark(device='cpu')\n",
    "\n",
    "# Run benchmark on primary model\n",
    "benchmark_results = benchmark.run_benchmark(\n",
    "    model=primary_model,\n",
    "    model_name='DemoCNN',\n",
    "    benchmark_level='basic',  # Use basic level for faster demo\n",
    "    calibrate=False,  # Skip calibration since we already did it\n",
    "    warmup_runs=1,\n",
    "    measure_memory=False  # Disable for CPU demo\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Display benchmark summary\n",
    "print(f\"‚úì Benchmark completed: {len(benchmark_results)} test configurations\")\n",
    "print(f\"‚úì Overall Success Rate: {benchmark_results['success_rate'].mean():.2%}\")\n",
    "print(f\"‚úì Average Confidence: {benchmark_results['confidence'].mean():.2%}\")\n",
    "print(f\"‚úì Average Throughput: {benchmark_results['throughput'].mean():.1f} challenges/sec\")\n",
    "print(f\"‚úì Total Benchmark Time: {benchmark_results['total_time'].sum():.2f} seconds\")\n",
    "\n",
    "# Display detailed results\n",
    "print(\"\\nDetailed Benchmark Results:\")\n",
    "display_cols = ['challenge_type', 'success_rate', 'confidence', 'throughput', 'verified']\n",
    "if all(col in benchmark_results.columns for col in display_cols):\n",
    "    print(benchmark_results[display_cols].to_string(index=False, float_format='%.3f'))\n",
    "else:\n",
    "    print(benchmark_results.head().to_string(index=False))\n",
    "\n",
    "# Visualize benchmark results\n",
    "if len(benchmark_results) > 1:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Success rate by challenge type\n",
    "    benchmark_results.groupby('challenge_type')['success_rate'].mean().plot(\n",
    "        kind='bar', ax=ax1, color='skyblue'\n",
    "    )\n",
    "    ax1.set_title('Success Rate by Challenge Type')\n",
    "    ax1.set_ylabel('Success Rate')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Throughput comparison\n",
    "    benchmark_results.groupby('challenge_type')['throughput'].mean().plot(\n",
    "        kind='bar', ax=ax2, color='lightcoral'\n",
    "    )\n",
    "    ax2.set_title('Throughput by Challenge Type')\n",
    "    ax2.set_ylabel('Challenges/Second')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient data for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Model Comparison\n",
    "\n",
    "print(\"Comparing multiple models...\")\n",
    "\n",
    "# Compare both demo models\n",
    "comparison_results = benchmark.compare_models(\n",
    "    models=models_dict,\n",
    "    benchmark_level='basic',\n",
    "    calibrate=False,\n",
    "    warmup_runs=0,\n",
    "    measure_memory=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comparison summary\n",
    "comparison_summary = comparison_results.groupby('model_name').agg({\n",
    "    'success_rate': 'mean',\n",
    "    'confidence': 'mean',\n",
    "    'throughput': 'mean',\n",
    "    'verified': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "comparison_summary['overall_score'] = (\n",
    "    0.4 * comparison_summary['success_rate'] +\n",
    "    0.3 * comparison_summary['confidence'] +\n",
    "    0.2 * comparison_summary['throughput'] / comparison_summary['throughput'].max() +\n",
    "    0.1 * comparison_summary['verified']\n",
    ")\n",
    "\n",
    "# Sort by overall score\n",
    "comparison_summary = comparison_summary.sort_values('overall_score', ascending=False)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_summary.to_string())\n",
    "\n",
    "# Performance ranking\n",
    "print(\"\\nPerformance Ranking:\")\n",
    "for i, (model, row) in enumerate(comparison_summary.iterrows()):\n",
    "    print(f\"  {i+1}. {model}: {row['overall_score']:.3f} (overall score)\")\n",
    "\n",
    "# Visualize comparison\n",
    "if len(comparison_summary) > 1:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Success rate comparison\n",
    "    comparison_summary['success_rate'].plot(kind='bar', ax=axes[0], color='lightblue')\n",
    "    axes[0].set_title('Success Rate Comparison')\n",
    "    axes[0].set_ylabel('Success Rate')\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    \n",
    "    # Confidence comparison\n",
    "    comparison_summary['confidence'].plot(kind='bar', ax=axes[1], color='lightgreen')\n",
    "    axes[1].set_title('Confidence Comparison')\n",
    "    axes[1].set_ylabel('Average Confidence')\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    \n",
    "    # Throughput comparison\n",
    "    comparison_summary['throughput'].plot(kind='bar', ax=axes[2], color='lightsalmon')\n",
    "    axes[2].set_title('Throughput Comparison')\n",
    "    axes[2].set_ylabel('Challenges/Second')\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Single model - no comparison visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Robustness Evaluation\n",
    "\n",
    "print(\"Evaluating model robustness...\")\n",
    "\n",
    "# Create robustness evaluator\n",
    "evaluator = VisionRobustnessEvaluator(verifier, device='cpu')\n",
    "\n",
    "# Test noise robustness (reduced scale for demo)\n",
    "noise_results = evaluator.evaluate_noise_robustness(\n",
    "    noise_levels=[0.01, 0.05, 0.1],  # Reduced levels\n",
    "    num_trials=5,  # Reduced trials\n",
    "    challenge_types=['frequency']  # Single type for speed\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ROBUSTNESS EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"‚úì Noise robustness tests completed: {len(noise_results)}\")\n",
    "\n",
    "print(\"\\nNoise Robustness Results:\")\n",
    "noise_data = []\n",
    "for test_name, result in noise_results.items():\n",
    "    noise_level = test_name.split('_')[-1]\n",
    "    noise_data.append({\n",
    "        'Noise Level': noise_level,\n",
    "        'Success Rate': f\"{result.success_rate:.2%}\",\n",
    "        'Std Dev': f\"{result.std_dev:.2%}\",\n",
    "        'Robustness Score': f\"{result.robustness_score:.3f}\",\n",
    "        'Baseline': f\"{result.baseline_success:.2%}\"\n",
    "    })\n",
    "\n",
    "noise_df = pd.DataFrame(noise_data)\n",
    "print(noise_df.to_string(index=False))\n",
    "\n",
    "# Test transformation robustness (reduced scale)\n",
    "transform_results = evaluator.evaluate_transformation_robustness(\n",
    "    num_trials=3,  # Reduced trials\n",
    "    challenge_types=['frequency']  # Single type for speed\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Transformation robustness tests completed: {len(transform_results)}\")\n",
    "\n",
    "print(\"\\nTransformation Robustness (Top 5):\")\n",
    "# Sort by robustness score and show top 5\n",
    "sorted_transforms = sorted(\n",
    "    transform_results.items(), \n",
    "    key=lambda x: x[1].robustness_score, \n",
    "    reverse=True\n",
    ")[:5]\n",
    "\n",
    "for test_name, result in sorted_transforms:\n",
    "    transform_name = test_name.replace('frequency_', '').replace('_', ' ').title()\n",
    "    print(f\"  {transform_name}: {result.robustness_score:.3f} robustness score\")\n",
    "\n",
    "# Visualize robustness results\n",
    "if noise_results:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Noise robustness plot\n",
    "    noise_levels = [float(k.split('_')[-1]) for k in noise_results.keys()]\n",
    "    robustness_scores = [v.robustness_score for v in noise_results.values()]\n",
    "    \n",
    "    ax1.plot(noise_levels, robustness_scores, 'o-', linewidth=2, markersize=8, color='red')\n",
    "    ax1.set_xlabel('Noise Level')\n",
    "    ax1.set_ylabel('Robustness Score')\n",
    "    ax1.set_title('Noise Robustness')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # Transformation robustness plot\n",
    "    if transform_results:\n",
    "        transform_names = [k.replace('frequency_', '').replace('_', '\\n') for k in list(transform_results.keys())[:8]]\n",
    "        transform_scores = [v.robustness_score for v in list(transform_results.values())[:8]]\n",
    "        \n",
    "        bars = ax2.bar(range(len(transform_names)), transform_scores, color='steelblue', alpha=0.7)\n",
    "        ax2.set_xlabel('Transformation Type')\n",
    "        ax2.set_ylabel('Robustness Score')\n",
    "        ax2.set_title('Transformation Robustness')\n",
    "        ax2.set_xticks(range(len(transform_names)))\n",
    "        ax2.set_xticklabels(transform_names, rotation=45, ha='right', fontsize=8)\n",
    "        ax2.set_ylim([0, 1])\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, transform_scores):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{score:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Overall robustness summary\n",
    "all_robustness_scores = (\n",
    "    [v.robustness_score for v in noise_results.values()] +\n",
    "    [v.robustness_score for v in transform_results.values()]\n",
    ")\n",
    "\n",
    "if all_robustness_scores:\n",
    "    print(f\"\\nOverall Robustness Summary:\")\n",
    "    print(f\"  Average robustness score: {np.mean(all_robustness_scores):.3f}\")\n",
    "    print(f\"  Best robustness score: {max(all_robustness_scores):.3f}\")\n",
    "    print(f\"  Worst robustness score: {min(all_robustness_scores):.3f}\")\n",
    "    print(f\"  Robustness standard deviation: {np.std(all_robustness_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Generate Reports\n",
    "\n",
    "print(\"Generating comprehensive reports...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('/tmp/vision_demo_reports')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate benchmark report\n",
    "if 'comparison_results' in locals() and len(comparison_results) > 0:\n",
    "    benchmark_report = benchmark.generate_report(\n",
    "        results=comparison_results,\n",
    "        output_path=str(output_dir / 'benchmark_report.html'),\n",
    "        include_plots=False  # Disable for demo compatibility\n",
    "    )\n",
    "    print(f\"‚úì Benchmark report saved: {benchmark_report}\")\n",
    "else:\n",
    "    print(\"‚ö† No comparison results available for benchmark report\")\n",
    "\n",
    "# Generate robustness report\n",
    "if noise_results or transform_results:\n",
    "    all_robustness_results = {**noise_results, **transform_results}\n",
    "    robustness_report = evaluator.generate_robustness_report(\n",
    "        results=all_robustness_results,\n",
    "        output_path=str(output_dir / 'robustness_report.html')\n",
    "    )\n",
    "    print(f\"‚úì Robustness report saved: {robustness_report}\")\n",
    "else:\n",
    "    print(\"‚ö† No robustness results available for report\")\n",
    "\n",
    "# Save benchmark state for future analysis\n",
    "state_file = output_dir / 'benchmark_state.json'\n",
    "benchmark.save_benchmark_state(str(state_file))\n",
    "print(f\"‚úì Benchmark state saved: {state_file}\")\n",
    "\n",
    "# List generated files\n",
    "generated_files = list(output_dir.glob('*'))\n",
    "print(f\"\\n‚úì Generated {len(generated_files)} report files:\")\n",
    "for file in generated_files:\n",
    "    file_size = file.stat().st_size / 1024  # KB\n",
    "    print(f\"  - {file.name} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(f\"\\nüìÅ All reports saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Summary and Conclusions\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISION VERIFICATION DEMO SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Collect all results for summary\n",
    "summary_data = {\n",
    "    'Models Tested': len(models_dict),\n",
    "    'Verification Method': config.get('verification_method', 'batch'),\n",
    "    'Device Used': str(verifier.device),\n",
    "}\n",
    "\n",
    "# Add verification results\n",
    "if 'result' in locals():\n",
    "    summary_data.update({\n",
    "        'Primary Model Verified': '‚úì' if result['verified'] else '‚úó',\n",
    "        'Primary Model Confidence': f\"{result['confidence']:.2%}\",\n",
    "        'Primary Model Success Rate': f\"{result['success_rate']:.2%}\"\n",
    "    })\n",
    "\n",
    "# Add calibration results\n",
    "if 'calibration_stats' in locals():\n",
    "    summary_data['Calibration Challenge Types'] = len(calibration_stats)\n",
    "\n",
    "# Add benchmark results\n",
    "if 'comparison_results' in locals():\n",
    "    summary_data.update({\n",
    "        'Benchmark Tests Run': len(comparison_results),\n",
    "        'Average Success Rate': f\"{comparison_results['success_rate'].mean():.2%}\",\n",
    "        'Average Throughput': f\"{comparison_results['throughput'].mean():.1f} challenges/sec\"\n",
    "    })\n",
    "\n",
    "# Add robustness results\n",
    "if 'all_robustness_scores' in locals() and all_robustness_scores:\n",
    "    summary_data.update({\n",
    "        'Robustness Tests': len(all_robustness_scores),\n",
    "        'Average Robustness Score': f\"{np.mean(all_robustness_scores):.3f}\",\n",
    "        'Robustness Range': f\"{min(all_robustness_scores):.3f} - {max(all_robustness_scores):.3f}\"\n",
    "    })\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nDemo Execution Summary:\")\n",
    "for key, value in summary_data.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\nKey Insights:\")\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Verification insight\n",
    "if 'result' in locals():\n",
    "    if result['verified']:\n",
    "        insights.append(f\"‚úì Primary model successfully verified with {result['confidence']:.1%} confidence\")\n",
    "    else:\n",
    "        insights.append(f\"‚ö† Primary model failed verification (confidence: {result['confidence']:.1%})\")\n",
    "\n",
    "# Performance insight\n",
    "if 'comparison_results' in locals() and len(comparison_results) > 1:\n",
    "    best_model = comparison_summary.index[0] if 'comparison_summary' in locals() else 'Unknown'\n",
    "    insights.append(f\"üèÜ Best performing model: {best_model}\")\n",
    "\n",
    "# Robustness insight\n",
    "if 'all_robustness_scores' in locals() and all_robustness_scores:\n",
    "    avg_robustness = np.mean(all_robustness_scores)\n",
    "    if avg_robustness > 0.8:\n",
    "        insights.append(f\"üõ°Ô∏è Model shows high robustness (avg: {avg_robustness:.3f})\")\n",
    "    elif avg_robustness > 0.5:\n",
    "        insights.append(f\"‚ö° Model shows moderate robustness (avg: {avg_robustness:.3f})\")\n",
    "    else:\n",
    "        insights.append(f\"‚ö†Ô∏è Model shows low robustness (avg: {avg_robustness:.3f})\")\n",
    "\n",
    "# Challenge type insight\n",
    "if 'result' in locals() and 'results' in result and result['results']:\n",
    "    challenge_types = [r.get('challenge_type', 'unknown') for r in result['results']]\n",
    "    if challenge_types:\n",
    "        most_common = max(set(challenge_types), key=challenge_types.count)\n",
    "        insights.append(f\"üìä Most tested challenge type: {most_common}\")\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\nNext Steps for Production Use:\")\n",
    "next_steps = [\n",
    "    \"1. Run comprehensive benchmark with larger sample sizes\",\n",
    "    \"2. Perform calibration with 500+ samples for accurate statistics\",\n",
    "    \"3. Test on GPU hardware for improved performance\",\n",
    "    \"4. Evaluate on production models and real datasets\",\n",
    "    \"5. Set up automated verification pipelines\",\n",
    "    \"6. Configure monitoring and alerting for model changes\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì DEMO COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìÅ Reports and data saved in: {output_dir}\")\n",
    "print(\"üìñ For more information, see pot/vision/README.md\")\n",
    "print(\"üîó Documentation: https://pot-framework.readthedocs.io/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}