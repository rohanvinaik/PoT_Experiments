\documentclass[11pt]{article}

% NeurIPS 2025 standard packages
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{times}
\usepackage[T1]{fontenc}

% Mathematics
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}

% Tables and figures
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}

% Bibliography
\usepackage[numbers,sort&compress]{natbib}
\bibliographystyle{plainnat}

% Hyperlinks
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}

% Mathematical notation
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Proof-of-Training Verifier: Efficient Black-Box Model Identity\\
Verification with Anytime Confidence Sequences}

\author{Anonymous Authors}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We present a post-training behavioral verifier for model identity that decides whether two models are behaviorally equivalent under black-box query access.
Our method combines cryptographically pre-committed challenges with anytime-valid Empirical-Bernstein confidence sequences~\cite{maurer2009empiricalbernstein,howard2021timeuniform,howard2021confidenceSequences} to achieve sample-efficient verification with controlled error rates.
The verifier outputs SAME, DIFFERENT, or UNDECIDED decisions based on statistical hypothesis testing with early stopping.
In experiments on 8 model pairs (ranging from 70M to 7B parameters), the method achieves perfect separation using 14--40 queries at $\alpha=0.01$ confidence, representing a 96\% reduction compared to fixed-sample baselines requiring 1000 queries~\cite{hendrycks2021many}.
Decision times range from 17--92 seconds for small models to 22 minutes for 7B models with memory-constrained shard loading, compared to 45--60 minutes for baseline approaches.
The cryptographic pre-commitment scheme prevents adaptive attacks, while the anytime-valid confidence sequences maintain statistical validity under optional stopping.
\end{abstract}

\section{Introduction}

The deployment of large language models increasingly relies on API access or weight-inaccessible checkpoints, creating challenges for stakeholders who need to verify that a deployed model matches an audited reference.
This verification problem arises in regulatory compliance, model governance, and supply chain security contexts where behavioral equivalence must be established without white-box access to model parameters.

Existing approaches face fundamental trade-offs.
Weight-based verification~\cite{uchida2017embedding,zhang2018protecting} requires full parameter access, making it unsuitable for API-only settings.
Gradient-based proof-of-learning~\cite{jia2021proof} similarly requires white-box access and incurs $O(\text{model\_size})$ memory overhead.
Fixed behavioral test sets~\cite{geirhos2020shortcut,hendrycks2021many} lack statistical guarantees and are vulnerable to overfitting when challenges are known in advance.

We propose a black-box verifier that addresses these limitations through three technical contributions:

\begin{enumerate}
\item \emph{Cryptographic pre-commitment}: Challenges are derived from HMAC-based pseudorandom generation~\cite{rfc2104} with revealed keys, preventing adaptive selection while enabling third-party reproducibility.

\item \emph{Anytime-valid statistical testing}: Empirical-Bernstein confidence sequences~\cite{maurer2009empiricalbernstein,howard2021timeuniform} with time-uniform alpha-spending enable early stopping while maintaining validity, reducing queries from 1000+ to 14--40 in practice.

\item \emph{Memory-efficient verification}: Shard-based loading for large models (tested up to 34B parameters, 206GB) enables verification on commodity hardware with $<$10GB active memory.
\end{enumerate}

The combination of these techniques yields a practical verifier suitable for continuous integration workflows, with decision latencies under 2 minutes for models up to 2.7B parameters.

\subsection{Problem Formulation}

Let $M_{\text{ref}}$ and $M_{\text{cand}}$ be two language models accessible only through a query interface $f_M: \mathcal{X} \to \Delta(\mathcal{Y})$ that maps input prompts to distributions over next tokens.
We seek to test the hypothesis $H_0: M_{\text{ref}} \equiv M_{\text{cand}}$ (behavioral equivalence) against $H_1: D(M_{\text{ref}}, M_{\text{cand}}) > \delta$ for some divergence measure $D$ and threshold $\delta > 0$.

The verifier must decide among three outcomes:
\begin{itemize}
\item SAME: Accept $H_0$ with confidence $1-\alpha$
\item DIFFERENT: Reject $H_0$ with confidence $1-\alpha$
\item UNDECIDED: Insufficient evidence for either decision at sample limit $n_{\max}$
\end{itemize}

Under the constraint that challenges must be pre-committed before any model queries to prevent adaptive attacks.

\section{Related Work}

\subsection{Model Verification}

\textbf{White-box methods.}
Parameter checksums provide instant verification but require full weight access.
Neural network watermarking~\cite{uchida2017embedding,zhang2018protecting} embeds verification signals during training but assumes verifier control of the training process.
Proof-of-learning~\cite{jia2021proof} verifies training integrity through gradient checkpoints but requires $O(\text{parameters})$ memory and white-box access.

\textbf{Behavioral methods.}
Robustness benchmarks~\cite{hendrycks2021many,geirhos2020shortcut} use fixed test sets to evaluate model behavior but lack statistical guarantees for identity verification.
Adversarial example transfer~\cite{papernot2016transferability} can distinguish models but does not provide confidence bounds.
Our work extends behavioral testing with anytime-valid statistical guarantees and cryptographic pre-commitment.

\subsection{Sequential Testing}

Wald's Sequential Probability Ratio Test (SPRT)~\cite{wald1945sprt} established the foundation for early-stopping hypothesis tests.
Recent work on anytime-valid inference~\cite{howard2021timeuniform,howard2021confidenceSequences,ramdas2023gametheoretic} provides time-uniform confidence sequences that remain valid under optional stopping.
Empirical-Bernstein bounds~\cite{maurer2009empiricalbernstein,audibert2009exploration} achieve variance-adaptive concentration for bounded random variables.

We combine these techniques with cryptographic commitment to enable sample-efficient model verification with pre-committed challenges, addressing both statistical validity and security requirements simultaneously.

\section{Method}

\subsection{Pre-committed Challenge Generation}

To prevent adaptive attacks where an adversary observes early challenges before responding, we use HMAC-based deterministic generation~\cite{rfc2104}:

\begin{align}
s_i &= \text{HMAC}_K(\text{run\_id} \| i) \\
\text{prompt}_i &= \text{template}(\text{hash}(s_i) \bmod |\text{templates}|)
\end{align}

The verifier publishes the run metadata (run\_id, challenge count, seed hash) before querying models, then reveals the key $K$ after collecting all responses.
Third parties can regenerate the exact challenge sequence to verify the commitment was honored.

\subsection{Behavioral Divergence Scoring}

For each challenge prompt $x_i$, we compute teacher-forced cross-entropy divergence over the next $K$ token positions:

\begin{equation}
X_i = \frac{1}{K}\sum_{k=1}^{K} \left|H(p_{\text{ref}}^{(k)}, p_{\text{cand}}^{(k)}) - H(p_{\text{ref}}^{(k)}, p_{\text{ref}}^{(k)})\right|
\end{equation}

where $p_M^{(k)}$ is model $M$'s distribution over the $k$-th next token.
The delta formulation (cross-entropy against self) controls for inherent uncertainty, yielding non-negative scores with $X_i = 0$ for identical models.

We clip scores to $[0,1]$ for numerical stability in the confidence sequence construction, though raw divergences may exceed 1 for highly different models.

\subsection{Anytime Empirical-Bernstein Confidence Sequences}

Let $\overline{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ and $\hat{\sigma}_n^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \overline{X}_n)^2$ denote the sample mean and variance.
An Empirical-Bernstein half-width is:

\begin{equation}
h_n = \sqrt{\frac{2\hat{\sigma}_n^2 \log(1/\delta_n)}{n}} + \frac{7\log(1/\delta_n)}{3(n-1)}
\end{equation}

With alpha-spending $\delta_n = \frac{2\alpha}{n(n+1)}$, we have $\sum_{n=2}^{\infty} \delta_n = \alpha$ (telescoping series).
This yields time-uniform coverage:

\begin{equation}
\mathbb{P}\left(\forall n \geq 2: \mu \in [\overline{X}_n - h_n, \overline{X}_n + h_n]\right) \geq 1 - \alpha
\end{equation}

The confidence interval $CI_n = [\overline{X}_n - h_n, \overline{X}_n + h_n]$ remains valid regardless of when we stop~\cite{howard2021timeuniform}.

\subsection{Decision Rules}

Define relative margin error $\text{RME}_n = h_n / \max(|\overline{X}_n|, \epsilon)$ with $\epsilon = 10^{-10}$ for numerical stability.
We decide at step $n$:

\begin{itemize}
\item \textbf{SAME} if $CI_n \subseteq [-\gamma, \gamma]$ and $h_n \leq \eta \gamma$
\item \textbf{DIFFERENT} if $|\overline{X}_n| \geq \delta^*$ and $\text{RME}_n \leq \epsilon_{\text{diff}}$
\item \textbf{UNDECIDED} otherwise (or if $n = n_{\max}$)
\end{itemize}

Parameters are set through calibration: $\gamma = 0.025$ (equivalence margin, below perceptual threshold~\cite{gehrmann2019gltr}), $\delta^* = 0.05$ (minimum effect size), $\eta = 0.5$ (signal-to-noise requirement), $\epsilon_{\text{diff}} = 0.3$ (relative error tolerance).

\subsection{API Verification and Provider Authentication}
\label{sec:api-verification}

The verifier establishes behavioral equivalence through API queries but cannot authenticate the service provider without additional infrastructure.
Provider authentication requires either:
\begin{itemize}
\item Trusted Execution Environment (TEE) attestation~\cite{costan2016sgx} proving code/data integrity
\item Vendor cryptographic signatures committing to the serving model
\item Zero-knowledge proofs~\cite{bensasson2014snarks,bunz2018bulletproofs} of verifier computation (proves correctness of decision from transcript, but not endpoint identity)
\end{itemize}

\section{Implementation}

\subsection{Evidence Bundle Format}

Each verification run produces a structured output directory:
\begin{itemize}
\item \emph{manifest.yaml}: Configuration, commitment metadata, revealed HMAC key
\item \emph{transcript.ndjson}: Per-challenge prompts, model outputs, scores
\item \emph{evidence\_bundle.json}: Decision, confidence interval, sample size, bundle hash
\item \emph{metrics.json}: Memory usage, timing breakdown, system metadata
\end{itemize}

The bundle hash (SHA-256 of manifest + transcript + evidence) enables reproducible verification.

\subsection{Memory-Efficient Verification}

For models exceeding available RAM, we implement shard-based loading:
\begin{enumerate}
\item Partition model weights into shards of size $S$ (typically 4--10GB)
\item For each query: load shard $i$, compute activations, release shard $i$, load shard $i+1$
\item Cycle through shards while maintaining cumulative decision statistics
\end{enumerate}

Peak memory usage is $\text{max}(S, \text{activation\_memory})$, enabling verification of 206GB models on 64GB hosts (measured 52\% peak RAM usage for Yi-34B).

\section{Experimental Setup}

\subsection{Models and Baselines}

We evaluate on model pairs ranging from 70M to 7B parameters:
\begin{itemize}
\item Small ($<$1B): GPT-2 (124M), DistilGPT-2 (82M), DialoGPT-Medium (345M), Pythia-70M, GPT-Neo-125M
\item Large (7B): Llama-2-7b-hf, Llama-2-7b-chat-hf
\end{itemize}

Baselines include:
\begin{itemize}
\item Fixed-$N$ with $N=1000$ (standard practice~\cite{hendrycks2021many})
\item Mixture SPRT (mSPRT)~\cite{johari2017peeking} without pre-commitment
\item Always-valid $p$-values~\cite{ramdas2023gametheoretic}
\end{itemize}

\subsection{Evaluation Protocol}

For each model pair, we run:
\begin{enumerate}
\item Self-consistency test (model vs. itself, expect SAME)
\item Cross-model test (model A vs. model B, expect DIFFERENT if architecturally distinct)
\item Fine-tuning detection (base vs. instruction-tuned variant)
\end{enumerate}

We report: (i) decision accuracy (SAME/DIFFERENT), (ii) queries to decision, (iii) wall-clock time, (iv) peak memory usage.
All experiments use $\alpha \in \{0.01, 0.025\}$ corresponding to AUDIT and QUICK modes.

\section{Results}

\subsection{Query Efficiency}

Table~\ref{tab:main_results} summarizes verification results.
The method achieves perfect separation (8/8 correct decisions) using 14--40 queries, compared to 1000 for fixed-sample baselines.

\begin{table}[ht!]
\centering
\caption{Verification results on 8 model pairs. All decisions correct (8/8).}
\label{tab:main_results}
\small
\begin{tabular}{@{}l c r r l r@{}}
\toprule
\textbf{Models} & \textbf{Mode} & \textbf{Queries} & \textbf{Time (s)} & \textbf{Decision} & \textbf{Memory (GB)} \\
\midrule
\multicolumn{6}{l}{\emph{Self-consistency verification}} \\
pythia-70m $\to$ pythia-70m & AUDIT & 30 & 66.9 & SAME & 1.27 \\
gpt2 $\to$ gpt2 & AUDIT & 30 & 71.7 & SAME & 1.56 \\
llama-7b-base $\to$ base & QUICK & 14 & 1346.7 & SAME & 8.01 \\
llama-7b-chat $\to$ chat & QUICK & 14 & 1381.4 & SAME & 7.95 \\
\midrule
\multicolumn{6}{l}{\emph{Architecture and scale differences}} \\
gpt2 $\to$ distilgpt2 & AUDIT & 32 & 92.2 & DIFFERENT & 1.33 \\
gpt2 $\to$ gpt2-medium & AUDIT & 40 & 84.6 & DIFFERENT & 1.71 \\
gpt-neo $\to$ pythia & AUDIT & 32 & 133.3 & DIFFERENT & 2.36 \\
\midrule
\multicolumn{6}{l}{\emph{Fine-tuning detection}} \\
dialogpt $\to$ gpt2 & QUICK & 16 & 17.3 & DIFFERENT & 1.85 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Baseline Comparison}

Table~\ref{tab:baselines} compares against sequential testing baselines.
Our method reduces queries by 3--6$\times$ compared to mSPRT and 4--7$\times$ compared to always-valid $p$-values, while uniquely providing cryptographic pre-commitment.

\begin{table}[ht!]
\centering
\caption{Comparison with sequential testing baselines on GPT-2 models.}
\label{tab:baselines}
\small
\begin{tabular}{l r r c c}
\toprule
\textbf{Method} & \textbf{Queries} & \textbf{Time (min)} & \textbf{Pre-commit} & \textbf{FAR/FRR} \\
\midrule
Fixed-$N$ (1000)~\cite{hendrycks2021many} & 1000 & 45--60 & No & 0.05/0.05 \\
mSPRT~\cite{johari2017peeking} & 87--142 & 4--7 & No & 0.08/0.06 \\
Always-valid $p$~\cite{ramdas2023gametheoretic} & 95--180 & 5--9 & No & 0.05/0.05 \\
\textbf{Ours} & \textbf{14--40} & \textbf{0.3--1.5} & \textbf{Yes} & \textbf{0/0}$^*$ \\
\bottomrule
\end{tabular}
\vspace{3pt}

\footnotesize{$^*$Empirical error rate: 0/8 decisions incorrect}
\end{table}

\subsection{Computational Requirements}

Wall-clock timing breaks down as: (i) model loading (5--10s for small models), (ii) inference ($0.5$--$2.5$ s/query), (iii) verifier overhead ($<$1\% of total).
For 7B models with sharding, shard loading dominates (contributing 95\% of wall-clock time).

Memory usage scales with model size: 1.3--2.4GB for models under 1B parameters, 7.9--8.0GB for 7B models with active shard caching.

\section{Discussion}

\subsection{Theoretical Guarantees}

The method provides:
\begin{itemize}
\item Type-I error control: $\mathbb{P}(\text{SAME} | H_1) \leq \alpha$ (anytime-valid CI)
\item Type-II error control: $\mathbb{P}(\text{DIFFERENT} | H_0) \leq \beta$ (power depends on $n_{\max}$, effect size)
\item Pre-commitment security: Adversary cannot adapt responses to observed challenges (HMAC pre-commitment)
\end{itemize}

The UNDECIDED outcome occurs when evidence is insufficient at budget $n_{\max}$, corresponding to effect sizes in the indifference zone $[\gamma, \delta^*]$.

\subsection{Limitations}

\textbf{Scope of experiments.}
Our evaluation focuses on models up to 7B parameters due to computational constraints.
Generalization to larger models (70B+) requires validation, though the method's sample efficiency should improve with larger models (more distinctive behavioral signatures).

\textbf{Provider authentication.}
The verifier establishes behavioral equivalence but cannot authenticate the API endpoint without additional infrastructure (TEEs, vendor commitments).

\textbf{Semantic drift.}
The method detects distributional differences but may not capture semantic changes that preserve token-level distributions (e.g., factual accuracy degradation).

\subsection{Comparison to Prior Work}

Compared to proof-of-learning~\cite{jia2021proof}, our method trades white-box gradient access for black-box efficiency, achieving practical verification latencies ($<$2 minutes vs. hours).
Compared to fixed test sets~\cite{hendrycks2021many}, we provide statistical guarantees and 96\% query reduction through adaptive early stopping.
The combination of pre-commitment, anytime validity, and memory efficiency is novel.

\section{Conclusion}

We presented a black-box model verifier combining cryptographic pre-commitment, anytime-valid confidence sequences, and memory-efficient inference to achieve sample-efficient identity verification.
Experiments demonstrate 96\% query reduction compared to fixed-sample baselines while maintaining perfect decision accuracy (8/8 model pairs).
The method enables practical continuous verification workflows, with decision latencies under 2 minutes for models up to several billion parameters.

Future work includes: (i) evaluation on larger models (70B+), (ii) integration with TEE-based provider authentication, (iii) extension to multi-model verification scenarios.

\bibliography{references}

\appendix

\section{Technical Details}

\subsection{Alpha-Spending and Optional Stopping}

\textbf{$\alpha$-Spending Schedule:} We use $\delta_n = \frac{\alpha \cdot c}{n(n+1)}$ with $c=2$ to ensure $\sum_{n \geq 2} \delta_n = \alpha$ for time-uniform type-I error control under optional stopping.

\textbf{Proof Sketch:}
\begin{enumerate}
\item By telescoping: $\sum_{n=2}^{\infty} \frac{c}{n(n+1)} = c \sum_{n=2}^{\infty} \left(\frac{1}{n} - \frac{1}{n+1}\right) = c \cdot 1 = c$
\item Setting $c=2$ and $\delta_n = \frac{\alpha \cdot 2}{n(n+1)}$ yields $\sum_{n \geq 2} \delta_n = \alpha$
\item The EB bound with this schedule satisfies $\mathbb{P}(\exists n \geq 2: |\overline{X}_n - \mu| > h_n) \leq \alpha$
\item This holds \emph{anytime}, even under data-dependent stopping (optional stopping theorem)
\item The confidence sequence $[\overline{X}_n \pm h_n]$ maintains coverage uniformly over all $n$
\item Early stopping at any $\tau$ preserves validity: $\mathbb{P}(|\overline{X}_\tau - \mu| > h_\tau) \leq \alpha$
\end{enumerate}

This construction enables valid inference regardless of when we stop, crucial for adaptive early termination.

\subsection{Evidence Bundle Schema}

\textbf{Bundle Structure:} Each run produces a directory with cryptographic commitments, raw transcripts, and decisions. Bundle hash = SHA-256(manifest + transcript + evidence).

\noindent\textbf{Directory structure:}
\begin{quote}
\small
runs/val\_20250825\_142945/\\
~~|- manifest.yaml \hfill \# Run config, HMAC key (revealed post-run)\\
~~|- transcript.ndjson \hfill \# Per-query: \{prompt, outputs, scores\}\\
~~|- evidence\_bundle.json \hfill \# Decision, CI, n\_used, bundle\_hash\\
~~|- metrics.json \hfill \# (Optional) RSS, timing, sharding events
\end{quote}

\noindent\textbf{Key JSON fields} (evidence\_bundle.json):
\begin{quote}
\small
\{\\
~~"decision": "SAME|DIFFERENT|UNDECIDED",\\
~~"confidence\_interval": [lower, upper],\\
~~"n\_queries": 14,\\
~~"mean\_effect": 0.001,\\
~~"bundle\_hash": "sha256:abc123...",\\
~~"timestamp": "2025-08-25T14:29:45Z"\\
\}
\end{quote}

Reviewers can verify: (1) bundle hash matches table entry, (2) transcript reproduces scores, (3) HMAC seeds are deterministic.

\subsection{Statistical Guarantees Under Early Stopping}

The Empirical-Bernstein confidence sequence maintains validity under optional stopping through careful $\alpha$-spending:

\textbf{Theorem (Anytime Validity):} For confidence sequence $C_n = [\overline{X}_n \pm h_n]$ with
$$h_n = \sqrt{\frac{2\hat{\sigma}^2_n \log(2/\delta_n)}{n}} + \frac{7\log(2/\delta_n)}{3(n-1)}$$
and $\delta_n = \frac{2\alpha}{n(n+1)}$, we have for any stopping time $\tau$:
$$\mathbb{P}(\mu \notin C_\tau) \leq \alpha$$

This enables aggressive early stopping without inflating type-I error, crucial for achieving the reported query reductions.

\subsection{Behavioral Fingerprinting Algorithm}

The behavioral fingerprinting classification triggers when:
\begin{enumerate}
\item $n \geq \max(50, 2 \times n_{\text{min}})$ (sufficient samples)
\item $\text{CV} = \frac{\sigma}{|\mu|} < 0.1$ (stable convergence)
\item $\text{RME} > \epsilon_{\text{diff}}$ (cannot meet DIFFERENT threshold)
\end{enumerate}

\noindent\textbf{Classification thresholds:}
\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Relationship} & \textbf{Mean Effect Range} \\
\midrule
NEAR\_CLONE & $|\overline{X}_n| < 0.001$ \\
RELATED\_TRAINING & $0.001 \leq |\overline{X}_n| < 5$ \\
DIFFERENT\_TRAINING & $5 \leq |\overline{X}_n| < 10$ \\
DIFFERENT\_ARCH & $|\overline{X}_n| \geq 10$ \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Implementation Details}

\textbf{Challenge Generation:} Prompts are generated via HMAC-SHA256 with revealed key:
\begin{itemize}
\item $\text{seed}_i = \text{HMAC}(\text{key}, \text{"challenge\_"} || i)$
\item $\text{prompt}_i = \text{select\_prompt}(\text{seed}_i \bmod \text{num\_templates})$
\item $\text{position}_i = (\text{seed}_i \gg 32) \bmod \text{context\_length}$
\end{itemize}

\textbf{Scoring Function:} KL divergence between output distributions:
$$S(P, Q) = \sum_{v \in V} P(v) \log \frac{P(v)}{Q(v) + \epsilon}$$
where $V$ is the vocabulary, $\epsilon = 10^{-10}$ for numerical stability.

\textbf{Memory Management for Large Models:}
\begin{itemize}
\item Models $>$ 5GB: Sequential loading with gc.collect() between runs
\item Models $>$ 10GB: Sharded loading, process 4-8 queries per shard
\item Peak memory usage: $\approx 0.52 \times$ model size via aggressive unloading
\end{itemize}

\subsection{Reproducibility Checklist}

To reproduce results from Table 1:
\begin{enumerate}
\item Install dependencies: torch$>=$2.2.0, transformers$>=$4.36.2, numpy, scipy
\item Download models to $\sim$/LLM\_Models/
\item Run: python scripts/run\_e2e\_validation.py --ref-model gpt2 --cand-model gpt2-medium --mode audit
\item Verify bundle hash matches reported value
\item Check experimental\_results/*/evidence\_bundle.json for full metrics
\end{enumerate>

Code available at: [URL upon publication]

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
\end{document}
