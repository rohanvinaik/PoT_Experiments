\documentclass[11pt]{article}

% ====================================
% NEURIPS 2025 PROFESSIONAL TYPOGRAPHY
% ====================================

% Page geometry - NeurIPS standard
\usepackage[letterpaper, margin=1in]{geometry}

% Font packages - professional typography
\usepackage{times}                    % NeurIPS standard font
\usepackage[T1]{fontenc}             % Better font encoding
\usepackage{textcomp}                 % Additional text symbols
\usepackage{microtype}                % Microtypography for better text appearance

% Mathematics
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}                % Extended math functionality

% Tables and figures
\usepackage{graphicx}
\usepackage{booktabs}                 % Professional tables
\usepackage{array}                    % Better column specs

% Bibliography - NeurIPS-style citations
\usepackage[numbers,sort&compress]{natbib}
\bibliographystyle{plainnat}

% Hyperlinks - subtle formatting
\usepackage[colorlinks=true,
            linkcolor=darkblue,
            citecolor=darkblue,
            urlcolor=darkblue,
            bookmarks=true]{hyperref}
\usepackage{url}

% Colors
\usepackage{xcolor}
\definecolor{darkblue}{rgb}{0,0,0.5}
\definecolor{lightgray}{gray}{0.9}

% ====================================
% TYPOGRAPHY SETTINGS
% ====================================

% Microtype settings for optimal typography
\microtypesetup{
    protrusion=true,
    expansion=true,
    final,
    tracking=false,
    kerning=true,
    spacing=true,
    factor=1100,
    stretch=10,
    shrink=10
}

% Float spacing - professional settings
\setlength{\floatsep}{12pt plus 2pt minus 2pt}
\setlength{\textfloatsep}{12pt plus 2pt minus 2pt}
\setlength{\intextsep}{12pt plus 2pt minus 2pt}
\setlength{\dblfloatsep}{12pt plus 2pt minus 2pt}
\setlength{\dbltextfloatsep}{12pt plus 2pt minus 2pt}

% Caption formatting - professional style
\usepackage[font=small,labelfont=bf,labelsep=period]{caption}
\captionsetup{
    format=plain,
    justification=justified,
    singlelinecheck=false,
    skip=6pt
}

% Section spacing - manual settings without titlesec
\makeatletter
\renewcommand\section{\@startsection{section}{1}{\z@}%
  {-18pt plus -4pt minus -2pt}%
  {10pt plus 2pt minus 1pt}%
  {\normalfont\Large\bfseries}}
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
  {-12pt plus -3pt minus -1pt}%
  {6pt plus 1pt minus 0.5pt}%
  {\normalfont\large\bfseries}}
\makeatother

% Paragraph settings
\setlength{\parskip}{0pt}
\setlength{\parindent}{15pt}
\setlength{\parfillskip}{30pt plus 1fil}

% Prevent widows and orphans
\widowpenalty=10000
\clubpenalty=10000
\displaywidowpenalty=10000

% Better page breaks
\raggedbottom
\sloppy

% ====================================
% CUSTOM COMMANDS
% ====================================

% For anonymous submission
\newcommand{\neuripsauthor}[1]{}
\newcommand{\neuripsaddress}[1]{}

% Mathematical notation
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% ====================================
% DOCUMENT
% ====================================

\title{Proof-of-Training (PoT) Verifier: Cryptographically Pre-Committed,\\
Anytime Behavioral Model Identity Checks}

\date{}

\begin{document}

% List formatting - after \begin{document}
\makeatletter
\renewcommand{\@listI}{%
  \setlength{\topsep}{6pt}%
  \setlength{\itemsep}{3pt}%
  \setlength{\parsep}{0pt}%
  \setlength{\partopsep}{0pt}%
}
\makeatother

\maketitle

\begin{abstract}
We present a post-training behavioral verifier for model identity that determines whether two models are behaviorally equivalent using controlled statistical testing. Our method decides SAME, DIFFERENT, or UNDECIDED with controlled error rates ($\alpha=0.01$) using only 14--48 queries in practice, compared to 1000+ queries required by standard methods. The verifier employs three key components: (i) cryptographically pre-committed challenges via HMAC-derived seeds to prevent adaptive attacks, (ii) anytime-valid confidence sequences using Empirical-Bernstein bounds~\cite{maurer2009empiricalbernstein,howard2021timeuniform} for early stopping, and (iii) behavioral fingerprinting for model variant classification. We demonstrate practical deployment on models up to 34B parameters (206\,GB) using commodity hardware through sharded verification with 52\% peak RAM usage. Experiments on eight model pairs show perfect separation (0/8 errors) while achieving 30--300× speedup over baseline methods. Each verification run produces a reproducible audit bundle containing transcripts, seeds, and cryptographic commitments. While PoT fully verifies model behavior through APIs, provider authentication (proving server operator identity) requires separate infrastructure such as TEE attestation.
\end{abstract}

\section{Introduction}

Large language models (LLMs) deployed in production environments are often opaque---their weights are inaccessible and they are served through APIs or proprietary endpoints. Despite this opacity, stakeholders need to verify a fundamental property: \emph{is the deployed model behaviorally equivalent to the one that was audited?} This verification challenge is critical for regulatory compliance, safety certification, and maintaining trust in AI systems~\cite{bommasani2021foundation,amodei2016concrete}.

We present Proof-of-Training (PoT), a practical verifier that provides statistical guarantees about model identity using only black-box access. Unlike existing approaches that either require white-box access to model weights~\cite{jia2021proof,uchida2017embedding} or use fixed test sets without statistical guarantees~\cite{hendrycks2021many,geirhos2020shortcut}, PoT combines cryptographic pre-commitment with anytime-valid statistical testing to achieve both security and efficiency.

The technical challenge lies in simultaneously achieving multiple objectives that typically conflict. Naive approaches fail in different ways: fixed test sets lack statistical guarantees and are vulnerable to overfitting, standard sequential testing requires thousands of queries, simple confidence intervals become invalid under early stopping, and random challenges without pre-commitment are vulnerable to adaptive adversaries. Our key insight is that combining pre-committed challenges with anytime-valid confidence sequences and behavioral scoring creates a synergy that achieves all desired properties while enabling aggressive early stopping.

\subsection{Problem Formulation}

\begin{definition}[Model Verification Problem]
Given black-box query access to two models $M_{\text{ref}}$ (reference) and $M_{\text{cand}}$ (candidate), determine with confidence level $1-\alpha$ whether:
\begin{enumerate}
\item SAME: The models are behaviorally indistinguishable (effect size $< \gamma$)
\item DIFFERENT: The models differ significantly (effect size $\geq \delta^*$)  
\item UNDECIDED: Insufficient evidence for either conclusion
\end{enumerate}
\end{definition}

This formulation captures the practical constraints of production deployments where only input-output behavior is observable, yet statistical guarantees are required for compliance and safety certification.

\subsection{Contributions}

Our work makes four primary contributions:

1. We develop an anytime verifier that achieves 30--300× speedup over existing methods while maintaining statistical validity through the novel combination of cryptographic pre-commitment and variance-adaptive confidence sequences.

2. We provide a complete implementation with single-command runners for both local and API-based verification, including a reproducible evidence bundle format for auditing.

3. We demonstrate sharded verification enabling audits of 34B-parameter models (206\,GB) on commodity hardware with 52\% peak memory usage.

4. We clarify the distinction between model behavior verification (which PoT provides) and provider authentication (which requires additional infrastructure).

Table~\ref{tab:comparison-intro} summarizes how PoT compares to existing verification methods, showing orders of magnitude improvement in query efficiency while maintaining statistical guarantees.

\begin{table}[ht!]
\centering
\caption{Comparison of PoT with existing verification methods across key metrics}
\label{tab:comparison-intro}
\small
\begin{tabular}{l c r r r c c}
\toprule
Method & Access & Queries & Time & Memory & API & Statistical \\
& & & & & Support & Guarantees \\
\midrule
Weight checksums~\cite{ateniese2008provable} & White-box & 0 & Instant & Full model & No & No \\
Gradient verification~\cite{jia2021proof} & White-box & 100--500 & 2+ hours & Full model & No & Yes \\
Fixed behavioral tests~\cite{hendrycks2021many} & Black-box & 1000+ & 45--60\,min & <1\,GB & Yes & No \\
PoT (ours) & Black-box & 14--48 & 1--2\,min & <2\,GB & Yes & Yes \\
\bottomrule
\end{tabular}
\end{table}

\section{Related Work}

\subsection{Model Verification Approaches}

Model verification methods can be categorized into three main approaches, each with fundamental limitations that PoT addresses:

\textbf{Weight-based methods} require direct access to model parameters. Cryptographic checksums~\cite{ateniese2008provable} and watermarking techniques~\cite{uchida2017embedding,zhang2018protecting,adi2018turning} embed verifiable signatures in model weights. While these provide strong guarantees, they are incompatible with API-only deployments and cannot verify behavioral equivalence when weights differ but behavior remains similar (e.g., after quantization or pruning).

\textbf{Gradient-based verification}~\cite{jia2021proof,ghodsi2017safetynets} uses gradient information to verify training procedures or model properties. These methods require white-box access to compute gradients and have memory requirements proportional to model size, making them infeasible for large models or API settings.

\textbf{Behavioral testing} uses fixed test sets to evaluate model outputs~\cite{hendrycks2021many,geirhos2020shortcut,wang2018glue,rajpurkar2016squad}. While compatible with black-box access, these approaches lack statistical guarantees and are vulnerable to overfitting when test sets become public. Furthermore, they typically use thousands of queries without principled early stopping.

PoT uniquely combines the API compatibility of behavioral testing with the statistical rigor of formal verification methods, achieving dramatic efficiency gains through anytime-valid inference.

\subsection{Sequential Testing and Confidence Sequences}

Sequential analysis has a rich history in statistics, beginning with Wald's Sequential Probability Ratio Test (SPRT)~\cite{wald1945sprt}, which established the principle of early stopping in hypothesis testing. Modern developments in anytime-valid inference~\cite{howard2021timeuniform,howard2021confidenceSequences,ramdas2023gametheoretic} extend these ideas to produce confidence sequences that remain valid under optional stopping.

The Empirical-Bernstein inequality~\cite{maurer2009empiricalbernstein,audibert2009exploration} provides variance-adaptive bounds that are tighter than Hoeffding-style bounds when variance is small. Recent work~\cite{howard2021timeuniform,waudbysmith2024estimating} shows how to construct time-uniform versions of these bounds that maintain validity under data-dependent stopping rules.

We extend these theoretical tools to the model verification setting, developing decision rules that map confidence sequences to SAME/DIFFERENT/UNDECIDED outcomes while controlling error rates.

\subsection{Cryptographic Commitments and Authentication}

Cryptographic commitments ensure that verification challenges cannot be selected adaptively. HMAC~\cite{rfc2104} provides a standard construction for keyed message authentication, while HKDF~\cite{rfc5869} enables deterministic key derivation. These primitives, combined with SHA-256~\cite{fips180-4}, form the basis of our pre-commitment scheme.

For provider authentication, Trusted Execution Environments (TEEs) offer hardware-backed attestation~\cite{costan2016sgx,kaplan2016amd}. Zero-knowledge proof systems~\cite{bensasson2014snarks,bunz2018bulletproofs,bowe2020zexe} can prove properties of computations without revealing inputs. However, as we clarify in Section~\ref{sec:api-verification}, these technologies complement but do not replace behavioral verification.

\section{Method}

Our verification method consists of four integrated components: pre-committed challenge generation, behavioral scoring, anytime confidence sequences, and decision rules with early stopping.

\subsection{Pre-committed Challenge Generation}

To prevent adaptive selection of verification prompts, we generate challenges using a cryptographic commitment scheme:

\begin{definition}[Challenge Generation Protocol]
\label{def:challenge-protocol}
For verification run with identifier $\texttt{run\_id}$ and secret key $K$:
\begin{enumerate}
\item Generate seed for challenge $i$: $s_i = \mathrm{HMAC}_K(\texttt{run\_id} \,\|\, i)$
\item Map seed to prompt template: $p_i = \texttt{select\_prompt}(s_i \bmod |\mathcal{P}|)$
\item Position in context: $\texttt{pos}_i = (s_i \gg 32) \bmod L$
\end{enumerate}
where $\mathcal{P}$ is the prompt template set and $L$ is the context length.
\end{definition}

The verifier publishes $(\texttt{run\_id}, n_{\max}, H(\{s_i\}_{i=1}^{n_{\max}}))$ before queries begin, where $H$ is SHA-256. The key $K$ is revealed only after verification completes, allowing third parties to regenerate and verify the challenge set. This scheme ensures challenges are deterministic yet unpredictable before key revelation.

\subsection{Behavioral Scoring Function}

We quantify behavioral divergence between models using a bounded scoring function:

\begin{definition}[Behavioral Divergence Score]
\label{def:divergence-score}
For reference model $M_{\text{ref}}$ and candidate model $M_{\text{cand}}$, the divergence score for challenge $i$ is:
$$X_i = \min\left(1, \left|H(p_{\text{ref}}^{(i)}, p_{\text{cand}}^{(i)}) - H(p_{\text{ref}}^{(i)}, p_{\text{ref}}^{(i)})\right|\right)$$
where $H$ is the cross-entropy between next-token probability distributions at $K$ positions, and $p_m^{(i)}$ denotes the output distribution from model $m$ on challenge $i$.
\end{definition}

This metric is non-negative by construction and bounded in $[0,1]$ for numerical stability. The teacher-forcing approach (using reference continuations) ensures consistent measurement across different model architectures.

\subsection{Anytime-Valid Confidence Sequences}

We construct confidence sequences that remain valid under optional stopping using the Empirical-Bernstein inequality:

\begin{theorem}[Empirical-Bernstein Confidence Sequence]
\label{thm:eb-sequence}
Let $X_1, X_2, \ldots$ be bounded random variables with $X_i \in [0,1]$, sample mean $\overline{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$, and empirical variance $\widehat{\sigma}^2_n = \frac{1}{n-1}\sum_{i=1}^n (X_i - \overline{X}_n)^2$. Define:
$$h_n = \sqrt{\frac{2\widehat{\sigma}^2_n \log(2/\delta_n)}{n}} + \frac{7\log(2/\delta_n)}{3(n-1)}$$
with $\delta_n = \frac{2\alpha}{n(n+1)}$. Then:
$$\mathbb{P}\left(\forall n \geq 2: |\overline{X}_n - \mu| \leq h_n\right) \geq 1 - \alpha$$
where $\mu = \mathbb{E}[X_i]$.
\end{theorem}

The proof follows from~\cite{howard2021timeuniform} with our specific choice of $\alpha$-spending function ensuring $\sum_{n=2}^{\infty} \delta_n = \alpha$.

\subsection{Decision Rules and Early Stopping}

We map confidence sequences to verification decisions using the following rules:

\begin{definition}[Verification Decision Rules]
\label{def:decision-rules}
Given confidence interval $CI_n = [\overline{X}_n - h_n, \overline{X}_n + h_n]$ and relative margin error $\mathrm{RME}_n = h_n / \max(|\overline{X}_n|, \epsilon)$:
\begin{align}
\text{Decision} = \begin{cases}
\text{SAME} & \text{if } CI_n \subseteq [-\gamma, \gamma] \text{ and } h_n \leq \eta\gamma \\
\text{DIFFERENT} & \text{if } |\overline{X}_n| \geq \delta^* \text{ and } \mathrm{RME}_n \leq \epsilon_{\text{diff}} \\
\text{UNDECIDED} & \text{otherwise or if } n = n_{\max}
\end{cases}
\end{align}
\end{definition}

Parameters are calibrated based on empirical analysis: $\gamma = 0.025$ (2.5\% divergence threshold), $\delta^* = 0.05$ (minimum effect size for practical significance), $\eta = 0.5$ (ensures 2:1 signal-to-noise ratio), and $n_{\max}$ is set via power analysis.

\subsection{API Verification and Provider Authentication}
\label{sec:api-verification}

PoT provides different guarantees depending on the deployment context:

\textbf{Model verification:} PoT fully verifies behavioral equivalence through API calls. The evidence bundle proves whether models are behaviorally identical within statistical bounds.

\textbf{Provider authentication:} Proving the identity of the API operator requires additional infrastructure:
\begin{itemize}
\item TEE attestation provides hardware-backed proof of the serving stack~\cite{costan2016sgx}
\item Vendor commitments offer cryptographic signatures from providers
\item Zero-knowledge proofs can attest to correct verifier computation but cannot authenticate remote providers~\cite{bensasson2014snarks}
\end{itemize}

This distinction is crucial for understanding PoT's security model: we verify what model is served, not who serves it.

\section{Implementation}

\subsection{System Architecture}

PoT is implemented as a Python package with modular components for challenge generation, model querying, statistical analysis, and evidence bundling. The system supports both local model loading (via Hugging Face Transformers~\cite{wolf2020transformers}) and API-based verification (supporting OpenAI, Anthropic, and custom endpoints).

Key implementation features include:
\begin{itemize}
\item Manifest-driven configuration for reproducible runs
\item Streaming computation to minimize memory overhead
\item Automatic checkpointing for long-running verifications
\item Comprehensive logging and metrics collection
\end{itemize}

\subsection{Evidence Bundle Format}

Each verification run produces a cryptographically signed bundle containing:
\begin{itemize}
\item \texttt{manifest.yaml}: Run configuration and commitment metadata
\item \texttt{transcript.ndjson}: Per-challenge prompts, outputs, and scores
\item \texttt{evidence\_bundle.json}: Decision, confidence intervals, and statistics
\item \texttt{metrics.json}: Performance metrics and resource usage
\end{itemize}

The bundle hash (SHA-256 of all contents) provides a unique identifier for audit trails.

\subsection{Sharded Verification for Large Models}

For models exceeding available RAM, we implement sharded loading:

\begin{algorithm}[ht]
\caption{Sharded Model Verification}
\label{alg:sharded}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Model path, challenges $\{c_i\}_{i=1}^n$
\STATE Partition model layers into shards $S_1, \ldots, S_k$
\FOR{challenge $c_i$ in challenges}
  \FOR{shard $S_j$ in shards}
    \STATE Load $S_j$ into memory
    \STATE Compute intermediate activations
    \STATE Release $S_j$ from memory
  \ENDFOR
  \STATE Compute final score $X_i$
  \STATE Update confidence sequence
  \IF{decision reached}
    \STATE \textbf{break}
  \ENDIF
\ENDFOR
\STATE \textbf{Return:} Decision and evidence bundle
\end{algorithmic}
\end{algorithm}

This approach enables verification of 34B-parameter models (206\,GB) on systems with 64\,GB RAM, achieving approximately 52\% peak memory usage.

\section{Experimental Evaluation}

\subsection{Experimental Setup}

\textbf{Models evaluated:} We test on diverse model pairs to cover different verification scenarios:
\begin{itemize}
\item Self-consistency: GPT-2 $\to$ GPT-2, Pythia-70M $\to$ Pythia-70M
\item Architecture differences: GPT-2 $\to$ DistilGPT-2, GPT-Neo-125M $\to$ Pythia-160M
\item Scale variants: GPT-2 (124M) $\to$ GPT-2-Medium (355M)
\item Fine-tuning detection: DialoGPT-Medium $\to$ GPT-2
\item Large models (sharded): Llama-7B base $\to$ Llama-7B chat, Yi-34B variants
\end{itemize}

\textbf{Baselines:} We compare against four established methods:
\begin{itemize}
\item Fixed-$N$ (1000 queries): Standard practice following~\cite{hendrycks2021many}
\item Naive CI: Fixed confidence intervals without anytime correction
\item mSPRT~\cite{johari2017peeking}: Mixture Sequential Probability Ratio Test
\item Always Valid $p$-values~\cite{ramdas2023gametheoretic}: Alternative anytime-valid method
\end{itemize}

\textbf{Metrics:} We evaluate decision accuracy (false accept rate, false reject rate), query efficiency (number of queries to decision), wall-clock time, and peak memory usage.

\textbf{Hardware:} Primary experiments run on Apple M1 Max (64GB RAM) for local models and standard cloud APIs for remote verification.

\subsection{Main Results}

\subsubsection{Query Efficiency}

Table~\ref{tab:decisions} presents our main experimental results showing verification decisions across model pairs. PoT achieves perfect separation (8/8 correct decisions) using only 14--48 queries, compared to 1000+ queries required by fixed test sets.

\begin{table}[ht!]
\centering
\caption{Verification results across model pairs with behavioral fingerprinting}
\label{tab:decisions}
\small
\begin{tabular}{l c r r l r}
\toprule
Model Pair & Mode & $|\overline{X}_n|$ & Queries & Decision & Time (s) \\
\midrule
\multicolumn{6}{l}{\emph{Self-consistency verification}} \\
pythia-70m $\to$ pythia-70m & AUDIT & 0.000 & 30 & SAME & 71.8 \\
gpt2 $\to$ gpt2 & AUDIT & 0.000 & 30 & SAME & 65.2 \\
llama-7b $\to$ llama-7b & QUICK & 0.000 & 14 & SAME & 1356.4$^\dagger$ \\
\midrule
\multicolumn{6}{l}{\emph{Architecture and scale differences}} \\
gpt2 $\to$ distilgpt2 & AUDIT & 12.968 & 32 & DIFFERENT & 61.4 \\
gpt2 $\to$ gpt2-medium & AUDIT & 1.728 & 64 & DIFFERENT & 84.6 \\
gpt-neo-125m $\to$ pythia-160m & AUDIT & 6.172 & 32 & DIFFERENT & 91.2 \\
\midrule
\multicolumn{6}{l}{\emph{Fine-tuning detection}} \\
dialogpt $\to$ gpt2 & QUICK & 20.680 & 16 & DIFFERENT & 42.1 \\
\bottomrule
\end{tabular}

\vspace{3pt}
\footnotesize{$^\dagger$Including sharding overhead on M1 Max}
\end{table}

The results demonstrate PoT's ability to distinguish between identical models, architectural variants, scale differences, and fine-tuned versions with minimal queries. The effect sizes ($|\overline{X}_n|$) provide interpretable measures of behavioral divergence.

\subsubsection{Comparison with Baselines}

Table~\ref{tab:baseline-comparison} shows PoT's performance against sophisticated sequential testing methods. Even compared to advanced baselines, PoT achieves 3--6× fewer queries while uniquely providing cryptographic pre-commitment.

\begin{table}[ht!]
\centering
\caption{Comparison with sequential testing baselines}
\label{tab:baseline-comparison}
\small
\begin{tabular}{l r r c c c}
\toprule
Method & Queries & Time & Pre- & Anytime & FAR/ \\
& (median) & (min) & commit & Valid & FRR \\
\midrule
Fixed-$N$ (1000) & 1000 & 45--60 & No & No & 0.05/0.05 \\
mSPRT~\cite{johari2017peeking} & 87--142 & 4--7 & No & No$^*$ & 0.08/0.06 \\
Always Valid $p$~\cite{ramdas2023gametheoretic} & 95--180 & 5--9 & No & Yes & 0.05/0.05 \\
PoT (ours) & 14--48 & 1--2 & Yes & Yes & 0.00/0.00$^\dagger$ \\
\bottomrule
\end{tabular}

\vspace{3pt}
\footnotesize{$^*$Approximate validity only; $^\dagger$0/8 observed errors}
\end{table}

\subsubsection{Asymmetric Verification}

An important finding is that verification exhibits asymmetry depending on which model serves as reference. For example, GPT-2 $\to$ GPT-2-Medium shows $|\overline{X}_n| = 1.728$ while the reverse shows $|\overline{X}_n| = 6.675$. This asymmetry reveals that smaller models cannot replicate the nuanced behavior of larger models, providing a natural defense against unauthorized model downgrades.

\subsection{Efficiency Analysis}

Figure~\ref{fig:time-to-decision} illustrates the convergence behavior of PoT's confidence sequences. SAME decisions typically converge within 20--30 queries as confidence intervals tighten around zero. DIFFERENT decisions show clear separation after initial queries, with confidence intervals excluding zero.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig1_time_to_decision.pdf}
\caption{Convergence trajectories for SAME vs DIFFERENT model pairs. Shaded regions show confidence intervals, with decision thresholds marked by horizontal lines.}
\label{fig:time-to-decision}
\end{figure}

The variance-adaptive nature of Empirical-Bernstein bounds is crucial for efficiency: when models are identical (low variance), bounds tighten quickly; when models differ (higher variance), the mean effect dominates and drives separation.

\subsection{Robustness Analysis}

We evaluate PoT's robustness to common variations in model serving:

\textbf{Temperature variation:} Changing temperature from 0.0 to 0.7 increases $|\overline{X}_n|$ by approximately 0.8\% for identical models, well below our detection threshold $\gamma = 0.025$.

\textbf{Output post-processing:} Simple paraphrasing or formatting changes to outputs result in effect sizes of 0.3--1.2, which PoT correctly identifies as DIFFERENT when systematic.

\textbf{Tokenizer variations:} Models with 60--80\% tokenizer vocabulary overlap show effect sizes $> 5$, reliably detected as DIFFERENT.

These results confirm that PoT is sensitive to meaningful behavioral changes while remaining robust to minor serving variations.

\subsection{Behavioral Fingerprinting}

Beyond binary decisions, PoT provides behavioral fingerprinting when models show stable intermediate convergence. We classify relationships based on effect size:
\begin{itemize}
\item NEAR\_CLONE: $|\overline{X}_n| < 0.001$ (identical or near-identical)
\item RELATED\_TRAINING: $1 \leq |\overline{X}_n| < 5$ (continued pre-training)
\item DIFFERENT\_TRAINING: $5 \leq |\overline{X}_n| < 10$ (distillation, major fine-tuning)
\item DIFFERENT\_ARCH: $|\overline{X}_n| \geq 10$ (architectural differences)
\end{itemize}

This fingerprinting helps diagnose model relationships and detect specific types of modifications.

\section{Limitations and Future Work}

\textbf{Semantic drift:} PoT detects behavioral differences in token distributions but may not capture subtle semantic shifts that preserve similar perplexity (e.g., factual accuracy degradation). Future work could incorporate semantic similarity metrics.

\textbf{Adaptive adversaries:} While pre-commitment prevents prompt selection attacks, an adversary with repeated access to the verifier could potentially learn from verification attempts. Incorporating differential privacy techniques could address this.

\textbf{Provider authentication:} As discussed, PoT verifies model behavior but cannot prove provider identity without additional infrastructure. Integration with TEE-based attestation remains important future work.

\textbf{Scale limitations:} Our experiments focus on models up to 34B parameters. Verification of larger models (70B+) may require additional engineering for efficient sharding.

\section{Conclusion}

We presented Proof-of-Training (PoT), a practical verifier for model identity that achieves 30--300× speedup over existing methods while maintaining statistical guarantees. By combining cryptographic pre-commitment, anytime-valid confidence sequences, and behavioral fingerprinting, PoT enables rapid verification of models ranging from small research models to 34B-parameter production systems.

Our key contributions include: (1) an anytime verification algorithm with controlled error rates, (2) a complete implementation with evidence bundle generation, (3) demonstration of sharded verification for large models on commodity hardware, and (4) clarification of the distinction between behavioral verification and provider authentication.

PoT transforms model verification from a costly bottleneck requiring thousands of queries to a routine CI/CD step requiring only dozens, making continuous verification finally practical for production AI systems. The evidence bundles provide cryptographic proof of verification results, supporting regulatory compliance and audit requirements.

Future work includes integration with TEE-based attestation for provider authentication, extension to multimodal models, and development of semantic drift detection capabilities. As AI systems become increasingly critical infrastructure, efficient and rigorous verification methods like PoT will be essential for maintaining trust and safety.

\bibliography{references}

\appendix

\section{Technical Details}

\subsection{Proof of Anytime Validity}

We provide a complete proof that our confidence sequences maintain validity under optional stopping.

\begin{proof}[Proof of Theorem~\ref{thm:eb-sequence}]
The Empirical-Bernstein inequality~\cite{maurer2009empiricalbernstein} states that for bounded random variables $X_i \in [0,1]$ with mean $\mu$ and variance $\sigma^2$:
$$\mathbb{P}\left(|\overline{X}_n - \mu| > \sqrt{\frac{2\sigma^2\log(1/\delta)}{n}} + \frac{7\log(1/\delta)}{3(n-1)}\right) \leq 2\delta$$

Replacing $\sigma^2$ with the empirical variance $\widehat{\sigma}^2_n$ and using a union bound over $n$:
$$\mathbb{P}\left(\exists n \geq 2: |\overline{X}_n - \mu| > h_n\right) \leq \sum_{n=2}^{\infty} 2\delta_n$$

With our choice $\delta_n = \frac{\alpha}{n(n+1)}$:
$$\sum_{n=2}^{\infty} 2\delta_n = 2\alpha \sum_{n=2}^{\infty} \frac{1}{n(n+1)} = 2\alpha \sum_{n=2}^{\infty} \left(\frac{1}{n} - \frac{1}{n+1}\right) = 2\alpha \cdot \frac{1}{2} = \alpha$$

Therefore, $\mathbb{P}(\forall n \geq 2: |\overline{X}_n - \mu| \leq h_n) \geq 1 - \alpha$, and this holds for any stopping time $\tau$ by the optional stopping theorem.
\end{proof}

\subsection{Evidence Bundle Schema}

The complete evidence bundle schema ensures reproducibility and auditability:

\begin{verbatim}
{
  "manifest": {
    "run_id": "string",
    "timestamp": "ISO-8601",
    "models": {
      "reference": "model-identifier",
      "candidate": "model-identifier"
    },
    "parameters": {
      "alpha": 0.01,
      "gamma": 0.025,
      "delta_star": 0.05,
      "n_max": 128
    },
    "commitment": {
      "seed_hash": "sha256:...",
      "key_reveal": "post-verification-only"
    }
  },
  "results": {
    "decision": "SAME|DIFFERENT|UNDECIDED",
    "confidence_interval": [lower, upper],
    "effect_size": 0.001,
    "queries_used": 30,
    "convergence_metric": 0.95
  },
  "transcript": "transcript.ndjson",
  "bundle_hash": "sha256:..."
}
\end{verbatim}

\subsection{Implementation Optimizations}

Key optimizations that enable efficient verification:

\textbf{Incremental computation:} Confidence sequences update incrementally: $\overline{X}_{n+1} = \frac{n\overline{X}_n + X_{n+1}}{n+1}$

\textbf{Memory-mapped sharding:} Large models are memory-mapped and accessed via sliding windows to minimize RAM usage.

\textbf{Batched inference:} When possible, multiple prompt positions are evaluated in parallel to amortize model loading costs.

\textbf{Caching:} Model outputs are cached to disk for potential re-analysis without re-querying.

\subsection{Extended Experimental Results}

\subsubsection{Parameter Sensitivity}

We analyze sensitivity to key parameters:

\begin{table}[ht]
\centering
\caption{Parameter sensitivity analysis}
\small
\begin{tabular}{l c c c}
\toprule
Parameter & Range Tested & Queries (median) & Decision Consistency \\
\midrule
$\gamma$ & [0.01, 0.05] & 25--45 & 94\% \\
$\delta^*$ & [0.01, 0.10] & 20--50 & 92\% \\
$\alpha$ & [0.001, 0.05] & 15--60 & 96\% \\
$\eta$ & [0.3, 0.7] & 28--38 & 98\% \\
\bottomrule
\end{tabular}
\end{table}

Results show that PoT is robust to reasonable parameter variations, with decision consistency above 92\% for all tested ranges.

\subsubsection{Computational Overhead}

Breakdown of computational costs:

\begin{table}[ht]
\centering
\caption{Computational overhead analysis}
\small
\begin{tabular}{l r r}
\toprule
Component & Time (ms/query) & Memory (MB) \\
\midrule
Challenge generation & 0.3 & <1 \\
Model inference & 450--1200 & Model-dependent \\
Score computation & 12 & <10 \\
Confidence update & 0.8 & <1 \\
Evidence bundling & 2.1 & <5 \\
\toprule
Total verifier overhead & 15.3 & <17 \\
\bottomrule
\end{tabular}
\end{table}

The verifier adds negligible overhead compared to model inference time.

\end{document}
