#!/usr/bin/env python3
"""
Test script for Sequential Decision Logic

This script tests the SPRT implementation with comprehensive validation of:
- Sequential Probability Ratio Test mechanics
- Early stopping criteria and decision boundaries
- Confidence calculations and evidence tracking
- Visualization tools and performance analysis
- Paper-based configurations and preset validation
"""

import sys
import os
sys.path.insert(0, os.path.abspath('.'))

import numpy as np
import torch
import time
from typing import List, Dict, Any
from pathlib import Path

from pot.experiments.sequential_decision import (
    SequentialDecisionMaker, SPRTConfig, DecisionOutcome, StoppingReason,
    SequentialResult, SequentialVisualizer, PAPER_CONFIGS,
    create_sequential_decision_maker, run_sequential_experiment,
    analyze_sequential_performance, save_sequential_results
)

def test_sprt_config():
    """Test SPRT configuration validation."""
    print("üß™ Testing SPRT Configuration")
    print("-" * 40)
    
    # Valid config
    try:
        config = SPRTConfig(alpha=0.05, beta=0.05, h0=0.3, h1=0.8)
        print("‚úÖ Valid config creation: passed")
    except Exception as e:
        print(f"‚ùå Valid config failed: {e}")
        return False
    
    # Invalid alpha
    try:
        config = SPRTConfig(alpha=1.5, beta=0.05, h0=0.3, h1=0.8)
        print("‚ùå Invalid alpha should have failed")
        return False
    except ValueError:
        print("‚úÖ Invalid alpha validation: passed")
    except Exception as e:
        print(f"‚ùå Wrong exception for invalid alpha: {e}")
        return False
    
    # Invalid hypothesis order
    try:
        config = SPRTConfig(alpha=0.05, beta=0.05, h0=0.8, h1=0.3)
        print("‚ùå Invalid hypothesis order should have failed")
        return False
    except ValueError:
        print("‚úÖ Invalid hypothesis order validation: passed")
    except Exception as e:
        print(f"‚ùå Wrong exception for invalid hypotheses: {e}")
        return False
    
    return True

def test_boundary_calculation():
    """Test SPRT boundary calculation."""
    print("\nüßÆ Testing Boundary Calculation")
    print("-" * 40)
    
    config = SPRTConfig(alpha=0.05, beta=0.05, h0=0.5, h1=0.9)
    decision_maker = SequentialDecisionMaker(config)
    
    # Check boundary values
    expected_accept = np.log((1 - 0.05) / 0.05)  # ln(19)
    expected_reject = np.log(0.05 / (1 - 0.05))  # ln(1/19)
    
    print(f"Expected accept boundary: {expected_accept:.4f}")
    print(f"Calculated accept boundary: {decision_maker.boundary_accept:.4f}")
    print(f"Expected reject boundary: {expected_reject:.4f}")
    print(f"Calculated reject boundary: {decision_maker.boundary_reject:.4f}")
    
    # Verify calculations
    accept_diff = abs(decision_maker.boundary_accept - expected_accept)
    reject_diff = abs(decision_maker.boundary_reject - expected_reject)
    
    if accept_diff < 1e-6 and reject_diff < 1e-6:
        print("‚úÖ Boundary calculation: passed")
        return True
    else:
        print(f"‚ùå Boundary calculation: accept_diff={accept_diff}, reject_diff={reject_diff}")
        return False

def test_basic_decisions():
    """Test basic decision making."""
    print("\nüéØ Testing Basic Decisions")
    print("-" * 40)
    
    config = SPRTConfig(alpha=0.05, beta=0.05, h0=0.5, h1=0.9, max_queries=10)
    decision_maker = SequentialDecisionMaker(config)
    
    # Test continuing decisions
    for i in range(3):
        decision = decision_maker.update(True)
        print(f"Observation {i+1} (pass): LLR={decision_maker.log_likelihood_ratio:.4f}, "
              f"confidence={decision_maker.get_confidence():.4f}, decision={decision.value}")
        
        if decision != DecisionOutcome.CONTINUE and i < 2:
            print(f"‚ùå Expected CONTINUE but got {decision.value}")
            return False
    
    # Test state
    state = decision_maker.get_current_state()
    print(f"Current state: {state['observations']} obs, {state['passes']} passes, "
          f"{state['fails']} fails")
    
    # Check if we can reach a decision with many passes
    for i in range(10):
        decision = decision_maker.update(True)
        if decision != DecisionOutcome.CONTINUE:
            print(f"Decision reached after {decision_maker.cumulative_passes} passes: {decision.value}")
            break
    
    if decision_maker.is_complete:
        print("‚úÖ Basic decision making: passed")
        return True
    else:
        print("‚ùå Failed to reach decision")
        return False

def test_early_stopping():
    """Test early stopping criteria."""
    print("\n‚è±Ô∏è Testing Early Stopping")
    print("-" * 40)
    
    # Test max queries stopping
    config = SPRTConfig(alpha=0.01, beta=0.01, h0=0.5, h1=0.6, max_queries=5)
    decision_maker = SequentialDecisionMaker(config)
    
    # Add observations that won't reach decision boundary
    decisions = []
    for i in range(6):
        decision = decision_maker.update(i % 2 == 0)  # Alternating pass/fail
        decisions.append(decision)
        print(f"Observation {i+1}: decision={decision.value}")
        
        if decision != DecisionOutcome.CONTINUE:
            break
    
    if decision == DecisionOutcome.MAX_QUERIES:
        print("‚úÖ Max queries stopping: passed")
    else:
        print(f"‚ùå Expected MAX_QUERIES but got {decision.value}")
        return False
    
    # Test confidence threshold stopping
    config2 = SPRTConfig(alpha=0.05, beta=0.05, h0=0.5, h1=0.95, 
                        confidence_threshold=0.8, max_queries=50)
    decision_maker2 = SequentialDecisionMaker(config2)
    
    # Add many passes to build confidence
    for i in range(10):
        decision = decision_maker2.update(True)
        confidence = decision_maker2.get_confidence()
        print(f"Pass {i+1}: confidence={confidence:.4f}, decision={decision.value}")
        
        if decision != DecisionOutcome.CONTINUE:
            if decision == DecisionOutcome.ACCEPT and confidence >= 0.8:
                print("‚úÖ Confidence threshold stopping: passed")
                return True
            break
    
    print("‚ùå Confidence threshold stopping failed")
    return False

def test_confidence_calculation():
    """Test confidence calculation accuracy."""
    print("\nüìä Testing Confidence Calculation")
    print("-" * 40)
    
    config = SPRTConfig(alpha=0.05, beta=0.05, h0=0.3, h1=0.9)
    decision_maker = SequentialDecisionMaker(config)
    
    # Test initial confidence
    initial_confidence = decision_maker.get_confidence()
    print(f"Initial confidence: {initial_confidence:.4f}")
    
    if abs(initial_confidence - 0.0) > 1e-6:
        print(f"‚ùå Initial confidence should be 0.0, got {initial_confidence}")
        return False
    
    # Add observations and check confidence evolution
    expected_confidences = []
    for i in range(5):
        decision_maker.update(True)  # All passes
        confidence = decision_maker.get_confidence()
        
        # Manual calculation
        llr = decision_maker.log_likelihood_ratio
        expected_conf = 1.0 / (1.0 + np.exp(-llr))
        expected_confidences.append(expected_conf)
        
        print(f"After {i+1} passes: LLR={llr:.4f}, "
              f"confidence={confidence:.4f}, expected={expected_conf:.4f}")
        
        if abs(confidence - expected_conf) > 1e-6:
            print(f"‚ùå Confidence mismatch at step {i+1}")
            return False
    
    # Check monotonic increase with passes
    confidences = [decision_maker.decision_history[i].confidence 
                  for i in range(len(decision_maker.decision_history))]
    
    if all(confidences[i] <= confidences[i+1] for i in range(len(confidences)-1)):
        print("‚úÖ Confidence monotonically increases with passes")
    else:
        print("‚ùå Confidence should increase monotonically with passes")
        return False
    
    print("‚úÖ Confidence calculation: passed")
    return True

def test_preset_configurations():
    """Test paper-based preset configurations."""
    print("\nüìã Testing Preset Configurations")
    print("-" * 40)
    
    presets = ["conservative", "standard", "fast", "research"]
    
    for preset in presets:
        try:
            decision_maker = create_sequential_decision_maker(preset)
            config = decision_maker.config
            
            print(f"‚úÖ {preset}: Œ±={config.alpha}, Œ≤={config.beta}, "
                  f"h0={config.h0}, h1={config.h1}, max_queries={config.max_queries}")
            
            # Verify bounds make sense
            if not (0 < config.alpha < 1 and 0 < config.beta < 1):
                print(f"‚ùå {preset}: Invalid error rates")
                return False
            
            if not (0 < config.h0 < config.h1 < 1):
                print(f"‚ùå {preset}: Invalid hypotheses")
                return False
            
        except Exception as e:
            print(f"‚ùå {preset} preset failed: {e}")
            return False
    
    # Test invalid preset
    try:
        decision_maker = create_sequential_decision_maker("invalid")
        print("‚ùå Invalid preset should have failed")
        return False
    except ValueError:
        print("‚úÖ Invalid preset validation: passed")
    except Exception as e:
        print(f"‚ùå Wrong exception for invalid preset: {e}")
        return False
    
    return True

def test_run_experiment():
    """Test complete experiment execution."""
    print("\nüî¨ Testing Complete Experiment")
    print("-" * 40)
    
    decision_maker = create_sequential_decision_maker("standard")
    
    # Create test observations
    observations = [True, True, False, True, True, True, True, False, True, True]
    metadata_list = [{"challenge_id": f"test_{i}"} for i in range(len(observations))]
    
    # Run experiment
    result = run_sequential_experiment(decision_maker, observations, metadata_list)
    
    print(f"Final decision: {result.final_decision.value}")
    print(f"Stopping reason: {result.stopping_reason.value}")
    print(f"Total observations: {result.total_observations}")
    print(f"Final confidence: {result.final_confidence:.4f}")
    print(f"Total time: {result.total_time:.4f}s")
    print(f"Evidence for H1: {result.evidence_for_h1:.4f}")
    print(f"Evidence against H0: {result.evidence_against_h0:.4f}")
    
    # Verify result structure
    if not isinstance(result, SequentialResult):
        print("‚ùå Result is not SequentialResult instance")
        return False
    
    if result.total_observations != len(result.decision_history):
        print("‚ùå Observation count mismatch")
        return False
    
    if result.final_decision not in DecisionOutcome:
        print("‚ùå Invalid final decision")
        return False
    
    if result.stopping_reason not in StoppingReason:
        print("‚ùå Invalid stopping reason")
        return False
    
    print("‚úÖ Complete experiment: passed")
    return True

def test_performance_analysis():
    """Test performance analysis functionality."""
    print("\nüìà Testing Performance Analysis")
    print("-" * 40)
    
    # Generate multiple results
    results = []
    for i in range(10):
        decision_maker = create_sequential_decision_maker("fast")
        
        # Create different observation patterns
        if i % 3 == 0:
            observations = [True] * 8  # Mostly passes
        elif i % 3 == 1:
            observations = [False] * 8  # Mostly fails
        else:
            observations = [True, False] * 4  # Mixed
        
        result = run_sequential_experiment(decision_maker, observations)
        result.metadata["ground_truth"] = i % 2 == 0  # Alternate ground truth
        results.append(result)
    
    # Analyze performance
    analysis = analyze_sequential_performance(results)
    
    print(f"Analysis keys: {list(analysis.keys())}")
    print(f"Total experiments: {analysis['total_experiments']}")
    print(f"Average observations: {analysis['average_observations']:.2f}")
    print(f"Median observations: {analysis['median_observations']:.2f}")
    print(f"Average time: {analysis['average_time']:.4f}s")
    print(f"Decision counts: {analysis['decision_counts']}")
    print(f"Efficiency gain: {analysis['efficiency_gain']}")
    
    # Verify analysis
    if analysis['total_experiments'] != len(results):
        print("‚ùå Experiment count mismatch")
        return False
    
    if not 0 <= analysis['average_confidence'] <= 1:
        print("‚ùå Invalid average confidence")
        return False
    
    print("‚úÖ Performance analysis: passed")
    return True

def test_visualization():
    """Test visualization functionality."""
    print("\nüìä Testing Visualization")
    print("-" * 40)
    
    # Create test output directory
    output_dir = Path("test_sequential_plots")
    output_dir.mkdir(exist_ok=True)
    
    try:
        visualizer = SequentialVisualizer(output_dir)
        config = SPRTConfig(alpha=0.05, beta=0.05, h0=0.5, h1=0.9)
        
        # Test boundary plot
        boundary_plot = visualizer.plot_decision_boundary(config)
        if boundary_plot.exists():
            print(f"‚úÖ Decision boundary plot created: {boundary_plot}")
        else:
            print("‚ùå Decision boundary plot not created")
            return False
        
        # Create test result for confidence plot
        decision_maker = SequentialDecisionMaker(config)
        observations = [True, True, False, True, True, True, False, True, True]
        result = run_sequential_experiment(decision_maker, observations)
        
        # Test confidence evolution plot
        confidence_plot = visualizer.plot_confidence_evolution(result)
        if confidence_plot.exists():
            print(f"‚úÖ Confidence evolution plot created: {confidence_plot}")
        else:
            print("‚ùå Confidence evolution plot not created")
            return False
        
        # Test ROC curve (with mock data)
        results_for_roc = []
        for i in range(20):
            mock_result = SequentialResult(
                final_decision=DecisionOutcome.ACCEPT if i % 2 == 0 else DecisionOutcome.REJECT,
                stopping_reason=StoppingReason.DECISION_REACHED,
                total_observations=i + 5,
                final_confidence=0.8 if i % 2 == 0 else 0.2,
                total_time=1.0,
                decision_history=[],
                log_likelihood_ratio=2.0 if i % 2 == 0 else -2.0,
                evidence_for_h1=max(0, 2.0 if i % 2 == 0 else -2.0),
                evidence_against_h0=max(0, 2.0 if i % 2 == 0 else -2.0),
                metadata={"ground_truth": i % 2 == 0}
            )
            results_for_roc.append(mock_result)
        
        roc_plot = visualizer.plot_roc_curves(results_for_roc)
        if roc_plot.exists():
            print(f"‚úÖ ROC curves plot created: {roc_plot}")
        else:
            print("‚ùå ROC curves plot not created")
            return False
        
        print("‚úÖ Visualization: passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Visualization failed: {e}")
        return False

def test_serialization():
    """Test result serialization and saving."""
    print("\nüíæ Testing Serialization")
    print("-" * 40)
    
    # Create test results
    results = []
    for i in range(5):
        decision_maker = create_sequential_decision_maker("standard")
        observations = [True] * (i + 3)  # Different lengths
        result = run_sequential_experiment(decision_maker, observations)
        results.append(result)
    
    # Test saving
    output_path = Path("test_sequential_results.json")
    
    try:
        saved_path = save_sequential_results(results, output_path, include_history=True)
        
        if saved_path.exists():
            print(f"‚úÖ Results saved to: {saved_path}")
            
            # Verify file size is reasonable
            file_size = saved_path.stat().st_size
            print(f"File size: {file_size} bytes")
            
            if file_size > 100:  # Should have some content
                print("‚úÖ File has reasonable size")
            else:
                print("‚ùå File seems too small")
                return False
            
        else:
            print("‚ùå Results file not created")
            return False
        
        # Test result to_dict method
        result_dict = results[0].to_dict()
        required_fields = ["final_decision", "stopping_reason", "total_observations", 
                          "final_confidence", "total_time"]
        
        for field in required_fields:
            if field not in result_dict:
                print(f"‚ùå Missing field in serialization: {field}")
                return False
        
        print("‚úÖ Serialization: passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Serialization failed: {e}")
        return False

def test_integration():
    """Test integration with existing PoT framework."""
    print("\nüîó Testing Integration")
    print("-" * 40)
    
    try:
        # Test if we can import core PoT components
        from pot.core import challenge
        from pot.core import fingerprint
        
        print("‚úÖ Core PoT imports: passed")
        
        # Test creating decision maker with PoT-like parameters
        config = SPRTConfig(
            alpha=0.001,         # Similar to PoT's alpha
            beta=0.001,          # Similar to PoT's beta
            h0=0.01,            # Similar to PoT's tau_id
            h1=0.9,             # High legitimate model performance
            max_queries=100
        )
        
        decision_maker = SequentialDecisionMaker(config)
        print("‚úÖ PoT-compatible config: passed")
        
        # Simulate PoT-style verification
        observations = []
        for i in range(10):
            # Simulate challenge-response (mock)
            challenge_passed = np.random.random() > 0.3  # 70% pass rate
            observations.append(challenge_passed)
            
            decision = decision_maker.update(challenge_passed, 
                                           {"challenge_id": f"pot_challenge_{i}",
                                            "challenge_type": "vision:freq"})
            
            if decision != DecisionOutcome.CONTINUE:
                break
        
        if decision_maker.final_result:
            print(f"‚úÖ PoT-style verification: {decision_maker.final_result.final_decision.value}")
            return True
        else:
            print("‚ùå PoT-style verification failed")
            return False
            
    except ImportError as e:
        print(f"‚ö†Ô∏è  PoT core components not available: {e}")
        # This is ok for standalone testing
        return True
    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        return False

def main():
    """Run all sequential decision tests."""
    print("üß™ Sequential Decision Logic Test Suite")
    print("=" * 60)
    print("Testing SPRT implementation with early stopping criteria,")
    print("confidence tracking, and visualization tools.\n")
    
    test_functions = [
        test_sprt_config,
        test_boundary_calculation,
        test_basic_decisions,
        test_early_stopping,
        test_confidence_calculation,
        test_preset_configurations,
        test_run_experiment,
        test_performance_analysis,
        test_visualization,
        test_serialization,
        test_integration
    ]
    
    results = []
    for test_func in test_functions:
        try:
            result = test_func()
            results.append(result)
        except Exception as e:
            print(f"‚ùå {test_func.__name__} failed with exception: {e}")
            results.append(False)
    
    # Summary
    passed = sum(results)
    total = len(results)
    
    print(f"\nüìä Test Results")
    print("=" * 30)
    print(f"Passed: {passed}/{total}")
    print(f"Success Rate: {passed/total*100:.1f}%")
    
    if passed == total:
        print("\nüéâ All tests passed!")
        print("\n‚úÖ Key Features Verified:")
        print("   - Sequential Probability Ratio Test (SPRT)")
        print("   - Early stopping criteria (max queries, confidence, time)")
        print("   - Decision boundary calculation and validation")
        print("   - Confidence evolution tracking")
        print("   - Multiple preset configurations")
        print("   - Comprehensive visualization tools")
        print("   - Performance analysis and metrics")
        print("   - Result serialization and persistence")
        print("   - Integration with PoT framework")
        return True
    else:
        print("\n‚ùå Some tests failed!")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)