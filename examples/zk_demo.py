#!/usr/bin/env python3
"""
Complete example demonstrating ZK proof generation during model training.

This example shows:
1. Training a small model with SGD
2. Generating ZK proofs for each training step
3. Storing proofs on blockchain
4. Verifying the training history
5. Demonstrating LoRA fine-tuning with ZK proofs
"""

import sys
import time
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Any

sys.path.append(str(Path(__file__).parent.parent))

# Import PoT components
from pot.zk.prover import auto_prove_training_step
from pot.zk.parallel_prover import OptimizedLoRAProver
from pot.zk.proof_aggregation import ProofAggregator, BatchVerifier
from pot.zk.config_loader import set_mode, get_config
from pot.prototypes.training_provenance_auditor import TrainingProvenanceAuditor
from pot.testing.mock_blockchain import MockBlockchainClient
from pot.zk.metrics import get_monitor, record_proof_generation


class SimpleModel:
    """Simple neural network for demonstration."""
    
    def __init__(self, input_dim: int = 10, hidden_dim: int = 20, output_dim: int = 5):
        """Initialize model with random weights."""
        self.weights = {
            'W1': np.random.randn(input_dim, hidden_dim).astype(np.float32) * 0.1,
            'b1': np.zeros(hidden_dim, dtype=np.float32),
            'W2': np.random.randn(hidden_dim, output_dim).astype(np.float32) * 0.1,
            'b2': np.zeros(output_dim, dtype=np.float32)
        }
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
    
    def forward(self, X: np.ndarray) -> np.ndarray:
        """Forward pass through the network."""
        # Hidden layer
        h = X @ self.weights['W1'] + self.weights['b1']
        h = np.maximum(0, h)  # ReLU
        
        # Output layer
        out = h @ self.weights['W2'] + self.weights['b2']
        
        # Softmax
        exp_out = np.exp(out - np.max(out, axis=-1, keepdims=True))
        return exp_out / np.sum(exp_out, axis=-1, keepdims=True)
    
    def compute_loss(self, X: np.ndarray, y: np.ndarray) -> float:
        """Compute cross-entropy loss."""
        predictions = self.forward(X)
        # Add small epsilon for numerical stability
        predictions = np.clip(predictions, 1e-7, 1 - 1e-7)
        
        # Cross-entropy loss
        n = X.shape[0]
        loss = -np.sum(y * np.log(predictions)) / n
        return loss
    
    def backward(self, X: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:
        """Compute gradients using backpropagation."""
        n = X.shape[0]
        
        # Forward pass
        h = X @ self.weights['W1'] + self.weights['b1']
        h_activated = np.maximum(0, h)
        predictions = self.forward(X)
        
        # Backward pass
        d_output = (predictions - y) / n
        
        gradients = {}
        gradients['W2'] = h_activated.T @ d_output
        gradients['b2'] = np.sum(d_output, axis=0)
        
        d_hidden = d_output @ self.weights['W2'].T
        d_hidden[h <= 0] = 0  # ReLU derivative
        
        gradients['W1'] = X.T @ d_hidden
        gradients['b1'] = np.sum(d_hidden, axis=0)
        
        return gradients
    
    def train_step(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01) -> float:
        """Perform one training step."""
        # Compute gradients
        gradients = self.backward(X, y)
        
        # Update weights
        for key in self.weights:
            self.weights[key] -= learning_rate * gradients[key]
        
        # Return loss
        return self.compute_loss(X, y)
    
    def get_state(self) -> Dict[str, np.ndarray]:
        """Get current model state."""
        return self.weights.copy()
    
    def set_state(self, state: Dict[str, np.ndarray]):
        """Set model state."""
        self.weights = state.copy()


class LoRAAdapter:
    """LoRA adapter for fine-tuning."""
    
    def __init__(self, base_model: SimpleModel, rank: int = 4):
        """Initialize LoRA adapters."""
        self.base_model = base_model
        self.rank = rank
        
        # Create low-rank adapters for W2 (output layer)
        d_in = base_model.hidden_dim
        d_out = base_model.output_dim
        
        self.adapters = {
            'lora_A': np.random.randn(d_in, rank).astype(np.float32) * 0.01,
            'lora_B': np.zeros((rank, d_out), dtype=np.float32)
        }
        self.alpha = rank * 2.0
    
    def forward(self, X: np.ndarray) -> np.ndarray:
        """Forward pass with LoRA."""
        # Base model hidden layer
        h = X @ self.base_model.weights['W1'] + self.base_model.weights['b1']
        h = np.maximum(0, h)
        
        # Output with LoRA
        base_out = h @ self.base_model.weights['W2'] + self.base_model.weights['b2']
        lora_out = h @ (self.adapters['lora_A'] @ self.adapters['lora_B']) * (self.alpha / self.rank)
        out = base_out + lora_out
        
        # Softmax
        exp_out = np.exp(out - np.max(out, axis=-1, keepdims=True))
        return exp_out / np.sum(exp_out, axis=-1, keepdims=True)
    
    def train_step(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.001) -> float:
        """Train only LoRA adapters."""
        n = X.shape[0]
        
        # Forward pass
        h = X @ self.base_model.weights['W1'] + self.base_model.weights['b1']
        h_activated = np.maximum(0, h)
        predictions = self.forward(X)
        
        # Compute loss
        predictions_clipped = np.clip(predictions, 1e-7, 1 - 1e-7)
        loss = -np.sum(y * np.log(predictions_clipped)) / n
        
        # Backward pass for LoRA
        d_output = (predictions - y) / n
        
        # Gradients for LoRA adapters
        scale = self.alpha / self.rank
        d_lora = h_activated.T @ d_output * scale
        
        grad_A = d_lora @ self.adapters['lora_B'].T
        grad_B = self.adapters['lora_A'].T @ d_lora
        
        # Update adapters
        self.adapters['lora_A'] -= learning_rate * grad_A
        self.adapters['lora_B'] -= learning_rate * grad_B
        
        return loss
    
    def get_state(self) -> Dict[str, np.ndarray]:
        """Get LoRA adapter state."""
        return {
            'lora_A.weight': self.adapters['lora_A'],
            'lora_B.weight': self.adapters['lora_B'],
            'base.weight': self.base_model.weights['W2']  # Include base for verification
        }


def generate_data(n_samples: int = 100, input_dim: int = 10, output_dim: int = 5) -> tuple:
    """Generate synthetic classification data."""
    X = np.random.randn(n_samples, input_dim).astype(np.float32)
    # One-hot encoded labels
    y_indices = np.random.randint(0, output_dim, n_samples)
    y = np.zeros((n_samples, output_dim), dtype=np.float32)
    y[np.arange(n_samples), y_indices] = 1
    return X, y


def train_with_zk_proofs():
    """Train model with ZK proof generation."""
    print("\n" + "="*60)
    print("TRAINING WITH ZK PROOFS")
    print("="*60)
    
    # Initialize components
    model = SimpleModel()
    blockchain = MockBlockchainClient()
    auditor = TrainingProvenanceAuditor(
        model_id="demo_model",
        hash_function="poseidon"
    )
    aggregator = ProofAggregator()
    
    # Set development mode for faster proofs
    set_mode('development')
    
    # Training configuration
    epochs = 3
    steps_per_epoch = 5
    batch_size = 32
    learning_rate = 0.01
    
    print(f"\nTraining Configuration:")
    print(f"  Epochs: {epochs}")
    print(f"  Steps per epoch: {steps_per_epoch}")
    print(f"  Batch size: {batch_size}")
    print(f"  Learning rate: {learning_rate}")
    
    all_proofs = []
    all_tx_hashes = []
    
    for epoch in range(epochs):
        print(f"\nüìö Epoch {epoch + 1}/{epochs}")
        epoch_proofs = []
        epoch_losses = []
        
        for step in range(steps_per_epoch):
            # Generate batch
            X_batch, y_batch = generate_data(batch_size, model.input_dim, model.output_dim)
            
            # Get model state before training
            state_before = model.get_state()
            
            # Training step
            loss = model.train_step(X_batch, y_batch, learning_rate)
            epoch_losses.append(loss)
            
            # Get model state after training
            state_after = model.get_state()
            
            # Generate ZK proof
            print(f"  Step {step + 1}: Generating ZK proof...", end="")
            start_time = time.time()
            
            proof_result = auto_prove_training_step(
                model_before=state_before,
                model_after=state_after,
                batch_data={'inputs': X_batch, 'targets': y_batch},
                learning_rate=learning_rate,
                step_number=epoch * steps_per_epoch + step,
                epoch=epoch
            )
            
            proof_time = (time.time() - start_time) * 1000
            
            if proof_result['success']:
                print(f" ‚úÖ ({proof_time:.0f}ms)")
                
                # Store on blockchain
                tx_hash = blockchain.store_commitment(proof_result['proof'])
                all_tx_hashes.append(tx_hash)
                epoch_proofs.append(proof_result['proof'])
                
                # Record metrics
                record_proof_generation(
                    proof_type=proof_result['proof_type'],
                    generation_time_ms=proof_time,
                    proof_size=len(proof_result['proof']),
                    success=True
                )
                
                # Log training event
                auditor.log_training_event(
                    epoch=epoch,
                    metrics={
                        'step': step,
                        'loss': float(loss),
                        'proof_hash': tx_hash
                    }
                )
            else:
                print(f" ‚ùå Failed")
        
        # Aggregate epoch proofs
        if epoch_proofs:
            from pot.zk.proof_aggregation import ProofBatch
            batch = ProofBatch(
                proofs=epoch_proofs,
                statements=[f"epoch_{epoch}_step_{i}" for i in range(len(epoch_proofs))],
                proof_type="sgd"
            )
            
            print(f"\n  Aggregating {len(epoch_proofs)} proofs...", end="")
            aggregated = aggregator.aggregate_proofs(batch)
            
            compression = sum(len(p) for p in epoch_proofs) / len(aggregated.proof_data)
            print(f" ‚úÖ (compression: {compression:.1f}x)")
            
            # Store aggregated proof
            agg_tx = blockchain.store_commitment(aggregated.proof_data)
            all_proofs.append(aggregated)
        
        # Print epoch summary
        avg_loss = np.mean(epoch_losses)
        print(f"  Epoch {epoch + 1} complete - Avg loss: {avg_loss:.4f}")
    
    print(f"\nüìä Training Summary:")
    print(f"  Total proofs generated: {len(all_tx_hashes)}")
    print(f"  All proofs stored on blockchain: ‚úÖ")
    
    # Verify training history
    print(f"\nüîç Verifying Training History...")
    is_valid = auditor.verify_training_history()
    print(f"  Training history valid: {'‚úÖ' if is_valid else '‚ùå'}")
    
    # Verify proof retrieval
    print(f"\nüîç Verifying Proof Storage...")
    for i, tx_hash in enumerate(all_tx_hashes[:3]):  # Check first 3
        proof = blockchain.get_commitment(tx_hash)
        status = blockchain.get_transaction_status(tx_hash)
        print(f"  Proof {i+1}: {'‚úÖ Retrieved' if proof else '‚ùå Not found'} (status: {status})")
    
    return model, all_tx_hashes


def finetune_with_lora():
    """Demonstrate LoRA fine-tuning with ZK proofs."""
    print("\n" + "="*60)
    print("LORA FINE-TUNING WITH ZK PROOFS")
    print("="*60)
    
    # Create base model
    base_model = SimpleModel()
    
    # Pre-train base model
    print("\nüìö Pre-training base model...")
    for _ in range(10):
        X, y = generate_data(32)
        base_model.train_step(X, y, learning_rate=0.01)
    
    # Create LoRA adapter
    lora = LoRAAdapter(base_model, rank=4)
    
    # Initialize ZK components
    blockchain = MockBlockchainClient()
    prover = OptimizedLoRAProver()
    prover.optimize_for_hardware()
    
    print(f"\nüîß LoRA Configuration:")
    print(f"  Rank: {lora.rank}")
    print(f"  Alpha: {lora.alpha}")
    print(f"  Parameters: {lora.adapters['lora_A'].size + lora.adapters['lora_B'].size}")
    print(f"  Compression vs full: {base_model.weights['W2'].size / (lora.adapters['lora_A'].size + lora.adapters['lora_B'].size):.1f}x")
    
    # Fine-tuning
    print(f"\nüìö Fine-tuning with LoRA...")
    
    for step in range(5):
        # Generate batch
        X_batch, y_batch = generate_data(32, base_model.input_dim, base_model.output_dim)
        
        # Get state before
        state_before = lora.get_state()
        
        # Training step
        loss = lora.train_step(X_batch, y_batch, learning_rate=0.001)
        
        # Get state after
        state_after = lora.get_state()
        
        # Generate ZK proof
        print(f"  Step {step + 1}: Loss={loss:.4f}, Generating proof...", end="")
        start_time = time.time()
        
        proof_result = auto_prove_training_step(
            model_before=state_before,
            model_after=state_after,
            batch_data={'inputs': X_batch, 'targets': y_batch},
            learning_rate=0.001
        )
        
        proof_time = (time.time() - start_time) * 1000
        
        if proof_result['success']:
            print(f" ‚úÖ ({proof_time:.0f}ms)")
            
            # Store on blockchain
            tx_hash = blockchain.store_commitment(proof_result['proof'])
            
            # Should detect LoRA
            assert proof_result['proof_type'] == 'lora', "Should detect LoRA training"
        else:
            print(f" ‚ùå Failed")
    
    print(f"\n‚úÖ LoRA fine-tuning complete with ZK proofs!")


def verify_proofs():
    """Demonstrate proof verification."""
    print("\n" + "="*60)
    print("PROOF VERIFICATION")
    print("="*60)
    
    # Create some proofs
    proofs = []
    statements = []
    
    for i in range(5):
        proof = f"proof_{i}".encode() * 32
        statement = f"statement_{i}"
        proofs.append(proof)
        statements.append(statement)
    
    # Batch verification
    verifier = BatchVerifier()
    
    print("\nüîç Verifying batch of proofs...")
    results = verifier.verify_batch(proofs, statements)
    
    for i, (stmt, result) in enumerate(zip(statements, results)):
        print(f"  {stmt}: {'‚úÖ Valid' if result else '‚ùå Invalid'}")
    
    # Show verification stats
    stats = verifier.get_stats()
    print(f"\nüìä Verification Statistics:")
    print(f"  Total verified: {stats['total_verified']}")
    print(f"  Average batch size: {stats.get('avg_batch_size', 0):.1f}")


def show_monitoring_dashboard():
    """Display monitoring dashboard."""
    print("\n" + "="*60)
    print("MONITORING DASHBOARD")
    print("="*60)
    
    monitor = get_monitor()
    dashboard = monitor.get_dashboard_data()
    summary = dashboard['summary']
    
    print(f"\nüìä Performance Metrics:")
    print(f"  Total proofs: {summary['total_proofs']}")
    print(f"  Success rate: {summary.get('proof_success_rate', 0):.1%}")
    print(f"  Avg proof time: {summary.get('avg_proof_time_ms', 0):.0f}ms")
    print(f"  Proofs/second: {summary.get('proofs_per_second', 0):.2f}")
    
    if summary.get('avg_cpu_percent'):
        print(f"\nüíª System Resources:")
        print(f"  CPU usage: {summary['avg_cpu_percent']:.1f}%")
        print(f"  Memory usage: {summary['avg_memory_mb']:.0f}MB")
    
    # Check for alerts
    alerts = dashboard.get('alerts', [])
    if alerts:
        print(f"\n‚ö†Ô∏è Active Alerts:")
        for alert in alerts[-3:]:
            print(f"  - {alert['message']}")


def main():
    """Run complete ZK proof demonstration."""
    print("\n" + "="*80)
    print(" "*20 + "ZK PROOF SYSTEM DEMONSTRATION")
    print("="*80)
    print("\nThis demo shows:")
    print("1. Training a model with ZK proof generation")
    print("2. Storing proofs on blockchain")
    print("3. LoRA fine-tuning with optimized proofs")
    print("4. Proof verification and monitoring")
    print("="*80)
    
    # Run demonstrations
    model, proof_hashes = train_with_zk_proofs()
    finetune_with_lora()
    verify_proofs()
    show_monitoring_dashboard()
    
    print("\n" + "="*80)
    print(" "*25 + "‚úÖ DEMO COMPLETE")
    print("="*80)
    print("\nKey Achievements:")
    print("  ‚úÖ Generated ZK proofs for all training steps")
    print("  ‚úÖ Stored proofs on blockchain")
    print("  ‚úÖ Detected and optimized LoRA training")
    print("  ‚úÖ Verified proof validity")
    print("  ‚úÖ Monitored performance metrics")
    print("\nThe ZK proof system is ready for production use!")
    print("="*80)


if __name__ == "__main__":
    main()