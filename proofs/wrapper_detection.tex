\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm,algorithm,algorithmic}
\usepackage{complexity}
\usepackage{hyperref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\title{Wrapper Detection in Neural Network Verification}
\author{Formal Analysis of Model Substitution Attacks}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We formally analyze the problem of detecting wrapper attacks where an 
adversary attempts to pass verification by wrapping a different model 
with input/output transformations. We prove bounds on the detectability 
of such wrappers under various threat models.
\end{abstract}

\section{Problem Formulation}

\begin{definition}[Wrapper Attack]
Given a target model $f_{\theta^*}: \mathcal{X} \rightarrow \mathcal{Y}$ that 
should pass verification, a wrapper attack consists of:
\begin{enumerate}
    \item A substitute model $g_{\phi}: \mathcal{X}' \rightarrow \mathcal{Y}'$
    \item An input transformation $T_{in}: \mathcal{X} \rightarrow \mathcal{X}'$
    \item An output transformation $T_{out}: \mathcal{Y}' \rightarrow \mathcal{Y}$
\end{enumerate}
Such that the composite function $\tilde{f} = T_{out} \circ g_{\phi} \circ T_{in}$ 
attempts to mimic $f_{\theta^*}$ on verification challenges.
\end{definition}

\begin{definition}[Wrapper Complexity]
The complexity of a wrapper $(T_{in}, T_{out})$ is defined as:
$$\mathcal{C}(T_{in}, T_{out}) = \mathcal{C}(T_{in}) + \mathcal{C}(T_{out})$$
where $\mathcal{C}(T)$ measures the computational complexity of transformation $T$.
\end{definition}

\section{Detection Theorem}

\begin{theorem}[Wrapper Detection Bound]
\label{thm:wrapper-detection}
Let $f_{\theta^*}$ be the target model and $\tilde{f} = T_{out} \circ g_{\phi} 
\circ T_{in}$ be a wrapped substitute. For a random challenge $c \sim \mathcal{D}$, 
the probability of detecting the wrapper is bounded by:
$$\Pr[\text{detect wrapper}] \geq 1 - \exp\left(-\frac{n \cdot d^2(f_{\theta^*}, g_{\phi})}{2\mathcal{C}(T_{in}, T_{out})}\right)$$
where $n$ is the challenge dimension and $d(f_{\theta^*}, g_{\phi})$ is the 
functional distance between the models.
\end{theorem}

\begin{proof}
We analyze the information-theoretic limits of wrapper concealment.

\textbf{Step 1: Information loss in transformations}

Any transformation $T$ with finite complexity cannot perfectly preserve all 
information. By the data processing inequality:
$$I(X; T(X)) \leq H(X)$$
with equality only if $T$ is bijective.

\textbf{Step 2: Challenge distinguishability}

For high-dimensional challenges $c \in \mathbb{R}^n$, the transformations 
must handle exponentially many possible inputs. The wrapper must satisfy:
$$\|f_{\theta^*}(c) - \tilde{f}(c)\| < \tau$$
for detection threshold $\tau$.

\textbf{Step 3: Complexity-accuracy trade-off}

By the Johnson-Lindenstrauss lemma, dimension-reducing transformations 
preserve distances only approximately. For $T_{in}: \mathbb{R}^n \rightarrow 
\mathbb{R}^{n'}$ with $n' < n$:
$$\Pr[\|T_{in}(c_1) - T_{in}(c_2)\| \approx \|c_1 - c_2\|] \leq \exp(-\Omega(n - n'))$$

\textbf{Step 4: Detection probability}

Combining the above, challenges that exploit the differences between 
$f_{\theta^*}$ and $g_{\phi}$ in regions poorly covered by the wrapper 
transformations lead to detection with probability:
$$\Pr[\text{detect}] \geq 1 - \exp\left(-\frac{n \cdot d^2(f_{\theta^*}, g_{\phi})}{2\mathcal{C}(T_{in}, T_{out})}\right)$$
\end{proof}

\section{Computational Hardness}

\begin{theorem}[Hardness of Perfect Wrapping]
\label{thm:wrapper-hardness}
Finding transformations $(T_{in}, T_{out})$ such that 
$T_{out} \circ g_{\phi} \circ T_{in} = f_{\theta^*}$ exactly is 
$\mathsf{NP}$-hard for general neural networks.
\end{theorem}

\begin{proof}
We reduce from the neural network equivalence problem.

Given two networks $f_1$ and $f_2$, determining if they compute the same 
function is known to be $\mathsf{NP}$-hard. 

If we could efficiently find perfect wrappers, we could:
\begin{enumerate}
\item Set $g_{\phi} = f_2$
\item Search for $(T_{in}, T_{out})$ such that $T_{out} \circ f_2 \circ T_{in} = f_1$
\item If found, conclude $f_1 \equiv f_2$ (with $T_{in} = T_{out} = I$)
\end{enumerate}

This would solve the equivalence problem in polynomial time, contradicting 
the $\mathsf{NP}$-hardness result.
\end{proof}

\section{Adaptive Challenge Construction}

\begin{theorem}[Adaptive Challenge Effectiveness]
\label{thm:adaptive}
Given oracle access to a wrapped model $\tilde{f}$, an adaptive challenge 
construction algorithm can detect wrappers with complexity 
$\mathcal{C}(T_{in}, T_{out}) \leq k$ using $O(k \log n)$ queries.
\end{theorem}

\begin{proof}
We use a binary search approach in challenge space.

\textbf{Algorithm:}
\begin{algorithmic}[1]
\STATE Initialize challenge set $S = \{c_1, ..., c_{\log n}\}$ randomly
\FOR{$i = 1$ to $k \log n$}
    \STATE Query $\tilde{f}(c_i)$ and $f_{\theta^*}(c_i)$
    \STATE Compute gradient $\nabla_c \|\tilde{f}(c) - f_{\theta^*}(c)\|$
    \STATE Update $c_{i+1} = c_i + \eta \nabla_c$
\ENDFOR
\STATE Return $\max_i \|\tilde{f}(c_i) - f_{\theta^*}(c_i)\|$
\end{algorithmic}

The gradient ascent finds regions where the wrapper fails to match the 
target model. With complexity bound $k$, the wrapper cannot perfectly 
track more than $O(k)$ adaptive adjustments.
\end{proof}

\section{Statistical Detection Methods}

\begin{proposition}[Statistical Fingerprinting]
The distribution of outputs $\{f(c_i)\}_{i=1}^N$ for random challenges 
provides a statistical fingerprint that distinguishes models with probability:
$$\Pr[\text{distinguish}] \geq 1 - \exp(-N \cdot \text{KL}(P_{f_{\theta^*}} \| P_{\tilde{f}}))$$
where $\text{KL}$ is the Kullback-Leibler divergence between output distributions.
\end{proposition}

\begin{proof}
By the Neyman-Pearson lemma, the likelihood ratio test is optimal for 
hypothesis testing. The test statistic:
$$\Lambda = \sum_{i=1}^N \log \frac{P_{f_{\theta^*}}(y_i)}{P_{\tilde{f}}(y_i)}$$
converges to $N \cdot \text{KL}(P_{f_{\theta^*}} \| P_{\tilde{f}})$ by the 
law of large numbers.

By Sanov's theorem, the probability of error decreases exponentially with $N$.
\end{proof}

\section{Timing-Based Detection}

\begin{theorem}[Timing Side-Channel Detection]
\label{thm:timing}
If the wrapper adds computational overhead $\Delta t > 0$ per query, then 
with $N$ timing measurements, the wrapper can be detected with probability:
$$\Pr[\text{detect via timing}] \geq 1 - \exp\left(-\frac{N \Delta t^2}{2\sigma^2}\right)$$
where $\sigma^2$ is the variance of timing measurements.
\end{theorem}

\begin{proof}
The timing measurements follow:
\begin{align}
T_{\text{original}} &\sim \mathcal{N}(\mu, \sigma^2) \\
T_{\text{wrapped}} &\sim \mathcal{N}(\mu + \Delta t, \sigma^2)
\end{align}

Using a two-sample t-test with $N$ samples from each distribution:
$$t = \frac{\bar{T}_{\text{wrapped}} - \bar{T}_{\text{original}}}{\sigma\sqrt{2/N}} \sim \mathcal{N}\left(\frac{\Delta t \sqrt{N}}{\sigma\sqrt{2}}, 1\right)$$

The power of the test (probability of detection) is:
$$\text{Power} = \Phi\left(\frac{\Delta t \sqrt{N}}{\sigma\sqrt{2}} - z_{\alpha/2}\right)$$

For large $N$, this approaches $1 - \exp(-N\Delta t^2/2\sigma^2)$.
\end{proof}

\section{Practical Implications}

\subsection{Detection Strategies}
Based on our theoretical analysis, effective wrapper detection combines:
\begin{enumerate}
    \item \textbf{High-dimensional challenges}: Exploit curse of dimensionality
    \item \textbf{Adaptive construction}: Iteratively find wrapper weaknesses
    \item \textbf{Statistical analysis}: Use distribution differences
    \item \textbf{Timing analysis}: Detect computational overhead
\end{enumerate}

\subsection{Security Recommendations}
To resist wrapper attacks:
\begin{itemize}
    \item Use challenges with dimension $n \geq 1000$
    \item Employ multiple detection methods simultaneously
    \item Monitor timing characteristics with precision $< 1$ms
    \item Maintain challenge unpredictability through cryptographic methods
\end{itemize}

\section{Conclusion}

Our analysis shows that wrapper attacks face fundamental theoretical limits. 
The detection probability increases with:
\begin{itemize}
    \item Challenge dimension $n$
    \item Functional distance between models
    \item Number of verification queries
\end{itemize}

Perfect wrapping is computationally intractable, and practical wrappers 
can be detected through multiple complementary methods.

\end{document}