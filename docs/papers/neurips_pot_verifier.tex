\documentclass{article}
% For submission, use the official NeurIPS 2025 style (no options).
% For arXiv/preprint, uncomment: \usepackage[preprint]{neurips_2025}
\usepackage{neurips_2025}

% -- Recommended packages (kept minimal) --
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{microtype}
\usepackage{xcolor}

\title{Proof-of-Training, Practically: A Sequential Behavioral Verifier with Cryptographic Auditability}

\author{%
  Anonymous Authors\\
  \texttt{anonymous@domain.tld}
}

\begin{document}
\maketitle

\begin{abstract}
We study the practical problem of verifying that a deployed model is \emph{the model you think it is}---without requiring access to its weights.
We present a \emph{sequential behavioral verifier} that (i) pre-commits a space of challenges via HMAC, (ii) streams model responses and maintains an \emph{Empirical-Bernstein} confidence sequence on a per-prompt effect-size statistic, and (iii) \emph{stops early} once explicit SAME/DIFFERENT criteria are met. The system emits a tamper-evident \emph{audit bundle} (seeds, transcript, thresholds, environment metadata) and optionally a small \emph{zero-knowledge proof} that the verifier computed the published decision over the published transcript. Empirically, we obtain decisive SAME/DIFFERENT calls in \emph{20--32 queries} on representative LM pairs (e.g., GPT-2 vs.\ DistilGPT-2; Llama-2-7B vs.\ Mistral-7B; Yi-34B vs.\ Yi-34B-Chat), with minutes-scale wall time. A sharded verifier validates $\sim$206~GB of models on a 64~GB host by loading/releasing slices sequentially. We discuss limitations (sample sizes, adversarial robustness, API attestation) and position this verifier as the post-training half of a broader PoT program, complementary to training-time provenance certificates.
\end{abstract}

\section{Introduction}
Deployed models are often opaque (API-only), rapidly updated, and valuable enough that misrepresentation (e.g., size-fraud, silent fine-tuning, substitution) is a real risk.
Weight-level attestation (e.g., TEE) is powerful but not always available, and training-time methods (e.g., watermarks) require control over training.
We ask: \emph{Can we verify model identity from behavior alone, quickly, with explicit error control---and without weights?}
Our answer is an anytime sequential test with pre-committed challenges. After each query, we update a variance-adaptive confidence sequence (Empirical-Bernstein, EB) over a bounded difference statistic and stop early when the interval certifies either SAME (small, well-localised difference) or DIFFERENT (large, stable effect). The verifier outputs a signed, reproducible transcript; in local-weights mode the transcript binds to a weight hash; in API mode it can be bound to a TEE attestation when available.

\paragraph{Contributions.}
(1) Operational verifier: SAME/DIFFERENT via EB confidence sequences and explicit stopping rules, typically in dozens of queries.
(2) Challenge integrity: HMAC-derived challenge generator preventing cherry-picking while preserving reproducibility.
(3) Auditability: evidence bundle (seeds, transcript, thresholds, environment) and optional ZK proof that the verifier computed the posted decision over the posted transcript.
(4) Frontier-scale feasibility: sharded runner verifying 34B-class models on commodity RAM.
(5) Scope clarity: delineation from training-time provenance and composition to end-to-end assurance.

\section{Related Work}
\textbf{Behavioral testing \& fingerprinting.} Fixed-$N$ behavioral comparisons require $10^3$--$10^4$ prompts and cannot stop early; our sequential design adapts to observed variance.
\textbf{Watermarking/ownership.} Requires training-time instrumentation; complementary to post-hoc verification.
\textbf{TEE attestation.} Strong identity binding when deployable; we use it as an optional binding layer for API mode.
\textbf{ZK proofs.} ZK can attest the verifier's computation over a transcript; binding the transcript to a specific remote model still requires an attested endpoint or vendor commitment.

\section{Method}
\subsection{Pre-committed challenges}
Let \texttt{run\_id} and HMAC key $K$ define challenge seeds
$\texttt{seed}_i=\mathrm{HMAC\_SHA256}(K, \texttt{run\_id}\,\|\, i)$.
Seeds deterministically parameterize a family of prompt templates and normalization settings. Publishing \texttt{run\_id}, counts, and the HMAC of the seed list pre-commits the challenge set.

\subsection{Streaming statistic and confidence sequence}
Each challenge yields a bounded per-prompt difference score $X_i\in[0,1]$ (e.g., normalized edit distance / fuzzy-hash dissimilarity over token sequences).
After $n$ prompts, with mean $\bar X_n$ and variance $S_n^2$ (Welford updates), the Empirical-Bernstein half-width is
\begin{equation}
U_n(\delta)=\sqrt{\tfrac{2S_n^2\log(1/\delta)}{n}}+\tfrac{7\log(1/\delta)}{3(n-1)},
\end{equation}
yielding an anytime confidence sequence via a spending schedule $\delta_n$.

\subsection{Decision rules}
Let the SAME-band be $[-\gamma,+\gamma]$ and $h_n=U_n(\delta_n)$.
\begin{itemize}
  \item \textbf{SAME} if $[\bar X_n-h_n,\ \bar X_n+h_n]\subseteq[-\gamma,+\gamma]$ and $h_n\le \eta\gamma$.
  \item \textbf{DIFFERENT} if $|\bar X_n|\ge \delta^\*$ and $h_n/|\bar X_n|\le \varepsilon_{\text{diff}}$.
  \item \textbf{UNDECIDED} otherwise; continue until $n_{\min}\le n\le n_{\max}$.
\end{itemize}

\subsection{Modes \& parameters}
\textsc{Quick}/\textsc{Audit}/\textsc{Extended} fix $\alpha$ (error budget), $\gamma,\eta,\delta^\*,\varepsilon_{\text{diff}}$, and $[n_{\min},n_{\max}]$. Defaults are conservative to favor \textsc{Undecided} over incorrect calls.

\subsection{Binding \& audit}
Local-weights: include SHA-256 of weight files; transcript binds to that hash.
API black-box: bind via TEE attestation (e.g., enclave quote) or vendor-signed model commitment.
ZK (prototype): Halo2 circuits attest ``the verifier computed decision $D$ from transcript $T$ using code hash $H$''. ZK does not identify the remote model by itself---binding is external.

\section{Implementation}
Core: EnhancedSequentialTester with EB updates and decision rules; HMAC challenge generator; token-space normalization; structured logging.
Runners: local HF models and API wrapper; sharded loader for 34B-class models (sequential load $\rightarrow$ evaluate $\rightarrow$ release).
Artifacts: logs (JSON), transcripts (NDJSON), seeds, thresholds, environment dump; evidence bundle packer and prototype ZK builder.

\section{Experiments}
\subsection{Setup}
Representative end-to-end runs on a single workstation (64~GB RAM). Deterministic decoding (e.g., temperature=0). We publish seeds, mode, thresholds, and transcripts.

\subsection{Pairs and results (illustrative)}
\begin{table}[t]
\centering
\begin{tabular}{lcccccl}
\toprule
Pair & Mode & Verdict & Queries & Total Time & Per-query & Notes \\
\midrule
Yi-34B vs Yi-34B-Chat & Audit & Different & 20 & $\sim$215 s & $\sim$10.7 s & Sharded verification \\
Llama-2-7B vs Mistral-7B & Audit & Different & 32 & $\sim$290 s & $\sim$9.1 s & Cross-architecture \\
GPT-2 vs DistilGPT-2 & Audit & Different & 32 & $\sim$56 s & $\sim$1.7 s & Distillation detection \\
Pythia-70M vs Pythia-70M & Quick & Same & 10 & $\sim$60 s & $\sim$6 s & Self-consistency \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Observation.} Decisions arrive in dozens of queries; minutes-scale wall time at 7B--34B; sharding keeps memory below host RAM.
We avoid FAR/FRR point estimates from tiny samples; instead we release transcripts and seeds and compute binomial CIs as more trials accumulate.

\section{Why it uses fewer queries}
Fixed-$N$ baselines (1k--10k prompts) cannot stop early. Our anytime EB sequence adapts to observed variance; when models are clearly the same (within $\gamma$) or clearly different (beyond $\delta^\*$), the interval collapses quickly and the decision stops---often after a few dozen challenges.

\section{Limitations}
Statistical power (small-$N$); adversarial robustness (wrapper attacks, sampling perturbations, tokenizer mismatch, MoE routing); binding in API mode (requires TEE or vendor commitments); scope (identity verification only).

\section{Composition with training-time PoT}
This verifier is the post-training half. The training-time half produces a provenance certificate (hash-chained checkpoints, signed environment, IO-evolution). Composition: bind deployment to the training certificate via weight hash or TEE, then run our behavioral verifier periodically to detect drift or substitution.

\section{Reproducibility checklist}
Code and requirements; per-run config (mode/thresholds); HMAC-seeded challenges and seed HMAC; transcripts; environment; binding (weight hash or attestation); optional ZK proof and evidence bundle.

\section{Ethics \& broader impact}
Enables provenance, accountability, and fraud prevention without exposing proprietary weights.
Not a safety/fairness guarantee; pair with governance and monitoring.

\section{Conclusion}
A sequential behavioral verifier with cryptographic pre-commitment and auditable transcripts decides SAME/DIFFERENT in dozens of queries and scales to frontier-size models via sharding. It composes with training-time provenance toward end-to-end Proof-of-Training.

\bibliographystyle{plainnat}
\bibliography{references}
\end{document}
