{
  "timestamp": 1755706410.648555,
  "results": [
    {
      "config": {
        "name": "BERT-base attention",
        "d_in": 768,
        "d_out": 768,
        "rank": 16,
        "description": "Typical BERT-base self-attention layer"
      },
      "full_params": 589824,
      "lora_params": 24576,
      "param_reduction": 24.0,
      "sgd_constraints": 2359486,
      "lora_constraints": 196748,
      "constraint_reduction": 11.992426860755891,
      "sgd_proof_time_ms": 335.9486,
      "lora_proof_time_ms": 119.6748,
      "speedup": 2.8071791220875237,
      "sgd_memory": 18874368,
      "lora_memory": 786432,
      "memory_reduction": 24.0,
      "sgd_proof_size": 4234,
      "lora_proof_size": 3517,
      "proof_size_reduction": 1.203866932044356
    },
    {
      "config": {
        "name": "BERT-large FFN",
        "d_in": 1024,
        "d_out": 4096,
        "rank": 32,
        "description": "BERT-large feed-forward network"
      },
      "full_params": 4194304,
      "lora_params": 163840,
      "param_reduction": 25.6,
      "sgd_constraints": 16777436,
      "lora_constraints": 1376426,
      "constraint_reduction": 12.189130400036035,
      "sgd_proof_time_ms": 1777.7436000000002,
      "lora_proof_time_ms": 237.6426,
      "speedup": 7.480744613970729,
      "sgd_memory": 134217728,
      "lora_memory": 5242880,
      "memory_reduction": 25.6,
      "sgd_proof_size": 4800,
      "lora_proof_size": 4078,
      "proof_size_reduction": 1.177047572339382
    },
    {
      "config": {
        "name": "GPT-2 medium",
        "d_in": 1024,
        "d_out": 1024,
        "rank": 24,
        "description": "GPT-2 medium attention layer"
      },
      "full_params": 1048576,
      "lora_params": 49152,
      "param_reduction": 21.333333333333332,
      "sgd_constraints": 4194504,
      "lora_constraints": 360598,
      "constraint_reduction": 11.632077826277461,
      "sgd_proof_time_ms": 519.4504,
      "lora_proof_time_ms": 136.0598,
      "speedup": 3.8178095219895956,
      "sgd_memory": 33554432,
      "lora_memory": 1572864,
      "memory_reduction": 21.333333333333332,
      "sgd_proof_size": 4400,
      "lora_proof_size": 3692,
      "proof_size_reduction": 1.191765980498375
    },
    {
      "config": {
        "name": "GPT-3 layer",
        "d_in": 4096,
        "d_out": 4096,
        "rank": 64,
        "description": "Large language model layer"
      },
      "full_params": 16777216,
      "lora_params": 524288,
      "param_reduction": 32.0,
      "sgd_constraints": 67109104,
      "lora_constraints": 5243070,
      "constraint_reduction": 12.799581924330592,
      "sgd_proof_time_ms": 6810.910400000001,
      "lora_proof_time_ms": 624.307,
      "speedup": 10.909553152535532,
      "sgd_memory": 536870912,
      "lora_memory": 16777216,
      "memory_reduction": 32.0,
      "sgd_proof_size": 5200,
      "lora_proof_size": 4464,
      "proof_size_reduction": 1.1648745519713262
    },
    {
      "config": {
        "name": "T5-large",
        "d_in": 1024,
        "d_out": 16384,
        "rank": 48,
        "description": "T5-large feed-forward layer"
      },
      "full_params": 16777216,
      "lora_params": 835584,
      "param_reduction": 20.07843137254902,
      "sgd_constraints": 67109104,
      "lora_constraints": 5865662,
      "constraint_reduction": 11.441011091331209,
      "sgd_proof_time_ms": 6810.910400000001,
      "lora_proof_time_ms": 686.5662000000001,
      "speedup": 9.92025299235529,
      "sgd_memory": 536870912,
      "lora_memory": 26738688,
      "memory_reduction": 20.07843137254902,
      "sgd_proof_size": 5200,
      "lora_proof_size": 4496,
      "proof_size_reduction": 1.1565836298932384
    },
    {
      "config": {
        "name": "XL model",
        "d_in": 8192,
        "d_out": 8192,
        "rank": 128,
        "description": "Extra-large model configuration"
      },
      "full_params": 67108864,
      "lora_params": 2097152,
      "param_reduction": 32.0,
      "sgd_constraints": 268435716,
      "lora_constraints": 20971730,
      "constraint_reduction": 12.79988422509731,
      "sgd_proof_time_ms": 26943.571600000003,
      "lora_proof_time_ms": 2197.1730000000002,
      "speedup": 12.262835743930951,
      "sgd_memory": 2147483648,
      "lora_memory": 67108864,
      "memory_reduction": 32.0,
      "sgd_proof_size": 5600,
      "lora_proof_size": 4864,
      "proof_size_reduction": 1.1513157894736843
    }
  ],
  "summary": {
    "avg_constraint_reduction": 12.142352054638083,
    "avg_speedup": 7.866395857811603,
    "avg_memory_reduction": 25.835294117647056,
    "avg_param_reduction": 25.835294117647056
  }
}