#!/usr/bin/env python3
"""
End-to-End Validation Pipeline CLI

This script provides a command-line interface for running the complete
PoT validation pipeline with comprehensive monitoring and reporting.
"""

import argparse
import sys
import os
import json
import time
from pathlib import Path
from datetime import datetime
import logging

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from src.pot.validation.e2e_pipeline import (
        PipelineOrchestrator,
        PipelineConfig,
        TestingMode,
        VerificationMode
    )
    from src.pot.validation.reporting import ReportGenerator
    
    # Import CI/CD components
    from benchmarks.tracking.performance_tracker import PerformanceTracker, PerformanceContext
    from benchmarks.tracking.dashboard_generator import DashboardGenerator
    from src.pot.audit.validation.audit_validator import AuditValidator
    from src.pot.audit.adversarial.attack_simulator import AttackSimulator, AttackScenario, AttackStrategy
    from src.pot.sharding.pipeline_integration import ShardedVerificationPipeline
    from scripts.ci.generate_evidence import CIEvidenceGenerator
    from tests.fixtures.test_data_manager import TestDataManager
    
    CI_COMPONENTS_AVAILABLE = True
except ImportError as e:
    print("Warning: Some CI/CD components not available:", str(e))
    CI_COMPONENTS_AVAILABLE = False
    try:
        from src.pot.validation.e2e_pipeline import (
            PipelineOrchestrator,
            PipelineConfig,
            TestingMode,
            VerificationMode
        )
        from src.pot.validation.reporting import ReportGenerator
    except ImportError:
        print("Error: Could not import core validation modules.")
        print("Please ensure you're running from the PoT_Experiments root directory.")
        sys.exit(1)


def setup_logging(verbose: bool = False):
    """Setup logging configuration"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    return logging.getLogger(__name__)


def parse_arguments():
    """Parse command-line arguments"""
    parser = argparse.ArgumentParser(
        description='Run end-to-end validation pipeline for PoT framework',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic validation with default settings
  %(prog)s --ref-model gpt2 --cand-model distilgpt2

  # Quick validation with dry run
  %(prog)s --ref-model gpt2 --cand-model distilgpt2 --mode quick --dry-run

  # Full audit with custom output directory
  %(prog)s --ref-model /path/to/model1 --cand-model /path/to/model2 \\
           --mode audit --output-dir results/my_validation

  # Benchmark mode with multiple runs
  %(prog)s --ref-model gpt2 --cand-model distilgpt2 --benchmark --benchmark-runs 5

  # API mode validation
  %(prog)s --ref-model http://api1.example.com --cand-model http://api2.example.com \\
           --verification-mode api --n-challenges 50
        """
    )
    
    # Required arguments
    parser.add_argument(
        '--ref-model',
        type=str,
        required=True,
        help='Path or identifier for reference model'
    )
    
    parser.add_argument(
        '--cand-model',
        type=str,
        required=True,
        help='Path or identifier for candidate model'
    )
    
    # Testing mode
    parser.add_argument(
        '--mode',
        type=str,
        choices=['quick', 'audit', 'extended'],
        default='audit',
        help='Testing mode (default: audit)'
    )
    
    # Verification mode
    parser.add_argument(
        '--verification-mode',
        type=str,
        choices=['local', 'api', 'hybrid'],
        default='local',
        help='Verification mode (default: local)'
    )
    
    # Pipeline options
    parser.add_argument(
        '--n-challenges',
        type=int,
        default=None,
        help='Max number of challenges (default: auto based on mode)'
    )
    
    parser.add_argument(
        '--max-queries',
        type=int,
        default=None,
        help='Maximum number of queries (default: from testing mode)'
    )
    
    parser.add_argument(
        '--hmac-key',
        type=str,
        default=None,
        help='HMAC key for challenge generation (auto-generated if not provided)'
    )
    
    # Output options
    parser.add_argument(
        '--output-dir',
        type=str,
        default='outputs/validation_reports',
        help='Output directory for reports (default: outputs/validation_reports)'
    )
    
    parser.add_argument(
        '--generate-report',
        action='store_true',
        default=True,
        help='Generate HTML report (default: True)'
    )
    
    parser.add_argument(
        '--no-report',
        dest='generate_report',
        action='store_false',
        help='Skip HTML report generation'
    )
    
    # Execution modes
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Run in dry-run mode (simulate without actual model loading)'
    )
    
    parser.add_argument(
        '--benchmark',
        action='store_true',
        help='Run in benchmark mode (multiple runs for performance testing)'
    )
    
    parser.add_argument(
        '--benchmark-runs',
        type=int,
        default=3,
        help='Number of benchmark runs (default: 3)'
    )
    
    # Performance options
    parser.add_argument(
        '--enable-zk',
        action='store_true',
        help='Enable ZK proof generation'
    )
    
    parser.add_argument(
        '--disable-memory-tracking',
        action='store_true',
        help='Disable memory tracking'
    )
    
    parser.add_argument(
        '--disable-cpu-tracking',
        action='store_true',
        help='Disable CPU tracking'
    )
    
    # CI/CD Integration options
    if CI_COMPONENTS_AVAILABLE:
        parser.add_argument(
            '--enable-performance-tracking',
            action='store_true',
            default=True,
            help='Enable comprehensive performance tracking (default: True)'
        )
        
        parser.add_argument(
            '--enable-audit-validation',
            action='store_true',
            default=True,
            help='Enable audit trail validation (default: True)'
        )
        
        parser.add_argument(
            '--enable-attack-simulation',
            action='store_true',
            help='Enable adversarial attack simulation'
        )
        
        parser.add_argument(
            '--enable-sharding',
            action='store_true',
            help='Enable model sharding for large models'
        )
        
        parser.add_argument(
            '--generate-evidence-bundle',
            action='store_true',
            default=True,
            help='Generate CI evidence bundle (default: True)'
        )
        
        parser.add_argument(
            '--performance-dashboard',
            action='store_true',
            help='Generate performance dashboard'
        )
        
        parser.add_argument(
            '--test-data-generation',
            action='store_true',
            help='Generate test data for CI testing'
        )
        
        parser.add_argument(
            '--update-readme-tables',
            action='store_true',
            default=True,
            help='Update README tables with results (default: True)'
        )
        
        parser.add_argument(
            '--no-readme-update',
            dest='update_readme_tables',
            action='store_false',
            help='Skip README table updates'
        )
    
    # Logging options
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose output'
    )
    
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='Suppress all output except errors'
    )
    
    # Config file option
    parser.add_argument(
        '--config',
        type=str,
        help='Load configuration from JSON file'
    )
    
    return parser.parse_args()


def load_config_file(config_path: str) -> dict:
    """Load configuration from JSON file"""
    try:
        with open(config_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading config file: {e}")
        return {}


def create_pipeline_config(args) -> PipelineConfig:
    """Create pipeline configuration from arguments"""
    # Map string mode to TestingMode enum
    mode_map = {
        'quick': TestingMode.QUICK_GATE,
        'audit': TestingMode.AUDIT_GRADE,
        'extended': TestingMode.AUDIT_GRADE  # Map extended to audit for now
    }
    
    # Map verification mode
    verification_map = {
        'local': VerificationMode.LOCAL_WEIGHTS,
        'api': VerificationMode.API_BLACK_BOX,
        'hybrid': VerificationMode.HYBRID
    }
    
    config = PipelineConfig(
        testing_mode=mode_map[args.mode],
        verification_mode=verification_map[args.verification_mode],
        enable_zk_proof=args.enable_zk,
        enable_memory_tracking=not args.disable_memory_tracking,
        enable_cpu_tracking=not args.disable_cpu_tracking,
        output_dir=Path(args.output_dir),
        dry_run=args.dry_run,
        benchmark_mode=args.benchmark,
        max_queries=args.max_queries,
        hmac_key=args.hmac_key,
        verbose=args.verbose and not args.quiet
    )
    
    return config


def print_summary(results: dict, logger: logging.Logger):
    """Print results summary to console"""
    if results.get('success'):
        logger.info("=" * 60)
        logger.info("VALIDATION COMPLETED SUCCESSFULLY")
        logger.info("=" * 60)
        logger.info(f"Decision: {results['decision']}")
        logger.info(f"Confidence: {results['confidence']:.4f}")
        logger.info(f"Queries Executed: {results['n_queries']}")
        logger.info(f"Total Duration: {results['total_duration']:.2f} seconds")
        logger.info(f"Peak Memory: {results['peak_memory_mb']:.2f} MB")
        logger.info("-" * 60)
        logger.info(f"Evidence Bundle: {results['evidence_bundle_path']}")
        if results.get('zk_proof_path'):
            logger.info(f"ZK Proof: {results['zk_proof_path']}")
    else:
        logger.error("=" * 60)
        logger.error("VALIDATION FAILED")
        logger.error("=" * 60)
        logger.error(f"Error: {results.get('error', 'Unknown error')}")
        logger.error(f"Failed at stage: {results.get('stage_failed', 'Unknown')}")


def run_enhanced_validation(args, logger, config, orchestrator):
    """Run enhanced validation with all CI/CD components"""
    start_time = time.time()
    results = {
        'components_used': [],
        'performance_data': {},
        'audit_data': {},
        'evidence_paths': []
    }
    
    # Initialize output directory
    output_dir = config.output_dir
    
    # Initialize performance tracking
    performance_tracker = None
    performance_session = None
    
    if CI_COMPONENTS_AVAILABLE and getattr(args, 'enable_performance_tracking', True):
        logger.info("üîß Initializing performance tracking...")
        performance_tracker = PerformanceTracker(str(output_dir / 'performance.db'))
        performance_session = performance_tracker.start_session(f"e2e_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        performance_tracker.start_monitoring(interval_seconds=1.0)
        results['components_used'].append('performance_tracking')
        
        # Record initial metrics
        performance_tracker.record_metric('validation_start_time', time.time(), 'timestamp', 'session')
        performance_tracker.record_metric('ref_model', args.ref_model, 'string', 'config', model_type='reference')
        performance_tracker.record_metric('cand_model', args.cand_model, 'string', 'config', model_type='candidate')
    
    try:
        # Check if sharding is needed
        if CI_COMPONENTS_AVAILABLE and getattr(args, 'enable_sharding', False):
            logger.info("üß© Checking if model sharding is needed...")
            sharded_pipeline = ShardedVerificationPipeline()
            
            should_shard_ref = sharded_pipeline.should_use_sharding(args.ref_model)
            should_shard_cand = sharded_pipeline.should_use_sharding(args.cand_model)
            
            if should_shard_ref or should_shard_cand:
                logger.info("üìä Large models detected - enabling sharding mode")
                results['components_used'].append('model_sharding')
                
                if performance_tracker:
                    performance_tracker.record_metric('sharding_enabled', 1, 'boolean', 'config')
                
                # Run sharded verification
                logger.info("‚öôÔ∏è Running sharded verification...")
                sharded_results = run_sharded_verification(args, sharded_pipeline, logger)
                results['sharded_verification'] = sharded_results
            else:
                logger.info("‚úÖ Models are suitable for standard verification")
        
        # Generate test data if requested
        if CI_COMPONENTS_AVAILABLE and getattr(args, 'test_data_generation', False):
            logger.info("üìã Generating test data...")
            test_data_manager = TestDataManager()
            test_env = test_data_manager.create_test_environment('e2e_validation_test')
            results['test_environment'] = test_env
            results['components_used'].append('test_data_generation')
        
        # Record validation start for performance tracking
        validation_start = time.time()
        
        # Return enhanced results with performance tracking data
        if performance_tracker:
            performance_tracker.record_timing('pre_validation_setup', (validation_start - start_time) * 1000)
        
        return results, performance_tracker
    
    except Exception as e:
        logger.error(f"‚ùå Enhanced validation setup failed: {e}")
        results['setup_error'] = str(e)
        return results, performance_tracker


def run_sharded_verification(args, sharded_pipeline, logger):
    """Run verification with sharding for large models"""
    try:
        # Prepare sharding configurations
        ref_shard_config, ref_shard_dir = sharded_pipeline.prepare_sharded_verification(
            args.ref_model, f"temp_shards/ref_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        
        cand_shard_config, cand_shard_dir = sharded_pipeline.prepare_sharded_verification(
            args.cand_model, f"temp_shards/cand_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        
        logger.info(f"üìä Sharding configuration:")
        logger.info(f"   Reference model: {ref_shard_config.num_shards} shards")
        logger.info(f"   Candidate model: {cand_shard_config.num_shards} shards")
        
        if getattr(args, 'dry_run', False):
            return {
                'success': True,
                'dry_run': True,
                'ref_shards': ref_shard_config.num_shards,
                'cand_shards': cand_shard_config.num_shards
            }
        
        # Return success result with sharding metadata
        return {
            'success': True,
            'ref_shard_config': {
                'num_shards': ref_shard_config.num_shards,
                'shard_size_mb': ref_shard_config.shard_size_mb
            },
            'cand_shard_config': {
                'num_shards': cand_shard_config.num_shards,
                'shard_size_mb': cand_shard_config.shard_size_mb
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Sharded verification failed: {e}")
        return {'success': False, 'error': str(e)}


def run_audit_validation(output_dir, logger):
    """Run audit trail validation"""
    try:
        validator = AuditValidator()
        
        # Look for audit files in output directory
        audit_files = list(output_dir.glob('*audit*.log'))
        
        if not audit_files:
            # Create a sample audit trail for demonstration
            sample_entries = [
                {
                    'timestamp': datetime.utcnow().isoformat(),
                    'operation': 'validation_start',
                    'actor': 'e2e_pipeline',
                    'resource': 'validation_session',
                    'outcome': 'success',
                    'hash': 'sample_hash_1',
                    'previous_hash': None
                }
            ]
            
            audit_file = output_dir / 'validation_audit.log'
            with open(audit_file, 'w') as f:
                for entry in sample_entries:
                    json.dump(entry, f)
                    f.write('\n')
            
            audit_files = [audit_file]
        
        validation_results = []
        for audit_file in audit_files:
            entries = validator.load_audit_trail(audit_file)
            result = validator.validate(entries)
            
            validation_results.append({
                'file': str(audit_file),
                'entries_count': len(entries),
                'is_valid': result.is_valid,
                'errors': len(result.errors),
                'warnings': len(result.warnings)
            })
        
        return {
            'success': True,
            'files_validated': len(audit_files),
            'entries_validated': sum(r['entries_count'] for r in validation_results),
            'validation_results': validation_results
        }
        
    except Exception as e:
        logger.error(f"‚ùå Audit validation failed: {e}")
        return {'success': False, 'error': str(e)}


def run_attack_simulation(args, logger):
    """Run adversarial attack simulation"""
    try:
        simulator = AttackSimulator()
        
        # Define attack scenarios
        scenarios = [
            AttackScenario(
                name="replay_attack_test",
                strategy=AttackStrategy.RANDOM,
                target="verification_system",
                objective="replay_previous_responses",
                constraints={'max_queries': 10},
                parameters={'replay_probability': 0.3}
            ),
            AttackScenario(
                name="timing_attack_test",
                strategy=AttackStrategy.ADAPTIVE,
                target="model_responses",
                objective="extract_timing_info",
                constraints={'max_queries': 5},
                parameters={'timing_threshold_ms': 100}
            )
        ]
        
        attack_outcomes = []
        for scenario in scenarios:
            # Create mock target for simulation
            class MockTarget:
                def process(self, query):
                    return {'success': True, 'detected': False, 'response_time_ms': 50}
            
            target = MockTarget()
            outcome = simulator.simulate_attack(scenario, target)
            
            attack_outcomes.append({
                'scenario_name': scenario.name,
                'success': outcome.success,
                'queries_used': outcome.queries_used,
                'detected': getattr(outcome, 'detected', False)
            })
        
        return {
            'success': True,
            'scenarios_tested': len(scenarios),
            'attack_outcomes': attack_outcomes
        }
        
    except Exception as e:
        logger.error(f"‚ùå Attack simulation failed: {e}")
        return {'success': False, 'error': str(e)}


def generate_evidence_bundle(output_dir, results, logger):
    """Generate CI evidence bundle"""
    try:
        evidence_generator = CIEvidenceGenerator(str(output_dir / 'evidence_bundle.zip'))
        
        # Generate evidence bundle with validation results
        bundle_path = evidence_generator.generate_evidence_bundle(
            commit_sha='local_validation',
            run_id=f"e2e_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        
        logger.info(f"üì¶ Evidence bundle generated: {bundle_path}")
        return bundle_path
        
    except Exception as e:
        logger.error(f"‚ùå Evidence bundle generation failed: {e}")
        return None


def generate_performance_dashboard(performance_tracker, output_dir, logger):
    """Generate performance dashboard"""
    try:
        dashboard_generator = DashboardGenerator(performance_tracker)
        dashboard_path = dashboard_generator.generate_dashboard(
            str(output_dir / 'dashboard'), 
            days=1
        )
        
        logger.info(f"üìä Performance dashboard generated: {dashboard_path}")
        return dashboard_path
        
    except Exception as e:
        logger.error(f"‚ùå Dashboard generation failed: {e}")
        return None


def print_enhanced_summary(results, logger):
    """Print enhanced results summary"""
    logger.info("=" * 80)
    logger.info("ENHANCED E2E VALIDATION RESULTS")
    logger.info("=" * 80)
    
    if results.get('success'):
        logger.info("‚úÖ Overall Status: SUCCESS")
        
        # Core validation results
        logger.info(f"üîç Decision: {results['decision']}")
        logger.info(f"üéØ Confidence: {results['confidence']:.4f}")
        logger.info(f"üìä Queries: {results['n_queries']}")
        
        # Enhanced components
        components = results.get('components_used', [])
        if components:
            logger.info(f"üõ†Ô∏è Enhanced Components Used: {', '.join(components)}")
        
        # Performance data
        if 'performance_data' in results:
            perf_data = results['performance_data']
            summary = perf_data.get('summary', {})
            logger.info(f"‚è±Ô∏è Total Duration: {results.get('total_duration'):.2f}s")
            logger.info(f"üîß Sessions: {summary.get('total_sessions', 0)}")
        
        # Evidence and outputs
        if 'evidence_bundle_path' in results:
            logger.info(f"üì¶ Evidence Bundle: {results['evidence_bundle_path']}")
        if 'dashboard_path' in results:
            logger.info(f"üìä Dashboard: {results['dashboard_path']}")
            
    else:
        logger.error("‚ùå Overall Status: FAILED")
        if 'error' in results:
            logger.error(f"üö® Error: {results['error']}")
    
    logger.info("=" * 80)


def main():
    """Enhanced main entry point with full CI/CD integration"""
    args = parse_arguments()
    
    # Setup logging
    logger = setup_logging(args.verbose and not args.quiet)
    
    # Load config from file if provided
    if args.config:
        file_config = load_config_file(args.config)
        # Merge with command-line arguments (CLI takes precedence)
        for key, value in file_config.items():
            if not hasattr(args, key) or getattr(args, key) is None:
                setattr(args, key, value)
    
    # Create pipeline configuration
    config = create_pipeline_config(args)
    
    # Print configuration summary
    if not args.quiet:
        logger.info("=" * 60)
        logger.info("POT E2E VALIDATION PIPELINE (UNIFIED)")
        logger.info("=" * 60)
        logger.info(f"Reference Model: {args.ref_model}")
        logger.info(f"Candidate Model: {args.cand_model}")
        logger.info(f"Testing Mode: {args.mode}")
        logger.info(f"Verification Mode: {args.verification_mode}")
        if args.n_challenges:
            logger.info(f"Max Challenges: {args.n_challenges}")
        else:
            logger.info(f"Max Challenges: Auto (from {args.mode} mode)")
        logger.info(f"Output Directory: {args.output_dir}")
        logger.info(f"üöÄ CI/CD Components Available: {CI_COMPONENTS_AVAILABLE}")
        
        if args.dry_run:
            logger.info(">>> DRY RUN MODE <<<")
        if args.benchmark:
            logger.info(f">>> BENCHMARK MODE ({args.benchmark_runs} runs) <<<")
        
        if CI_COMPONENTS_AVAILABLE:
            enhanced_features = []
            if getattr(args, 'enable_performance_tracking', True):
                enhanced_features.append('Performance Tracking')
            if getattr(args, 'enable_audit_validation', True):
                enhanced_features.append('Audit Validation')
            if getattr(args, 'enable_attack_simulation', False):
                enhanced_features.append('Attack Simulation')
            if getattr(args, 'enable_sharding', False):
                enhanced_features.append('Model Sharding')
            if getattr(args, 'generate_evidence_bundle', True):
                enhanced_features.append('Evidence Generation')
            if getattr(args, 'performance_dashboard', False):
                enhanced_features.append('Performance Dashboard')
            
            if enhanced_features:
                logger.info(f"üõ†Ô∏è Enhanced Features: {', '.join(enhanced_features)}")
        else:
            logger.warning("‚ö†Ô∏è Some CI/CD features disabled due to missing components")
        
        logger.info("=" * 60)
    
    try:
        # Create orchestrator
        orchestrator = PipelineOrchestrator(config)
        
        # Run enhanced validation setup
        enhanced_results, performance_tracker = run_enhanced_validation(args, logger, config, orchestrator)
        
        # Run pipeline
        if args.benchmark:
            # Benchmark mode
            logger.info(f"Starting benchmark with {args.benchmark_runs} runs...")
            results = orchestrator.benchmark_pipeline(
                ref_model_path=args.ref_model,
                cand_model_path=args.cand_model,
                n_runs=args.benchmark_runs
            )
            
            # Print benchmark summary
            if not args.quiet:
                logger.info("=" * 60)
                logger.info("BENCHMARK RESULTS")
                logger.info("=" * 60)
                logger.info(f"Successful Runs: {results['n_successful']}/{results['n_runs']}")
                if results['n_successful'] > 0:
                    logger.info(f"Average Duration: {results['avg_duration_seconds']:.2f} seconds")
                    logger.info(f"Average Peak Memory: {results['avg_peak_memory_mb']:.2f} MB")
                    logger.info(f"Average Queries: {results['avg_queries']:.1f}")
        else:
            # Normal mode with enhanced validation
            results = orchestrator.run_complete_pipeline(
                ref_model_path=args.ref_model,
                cand_model_path=args.cand_model,
                n_challenges=args.n_challenges
            )
            
            # Integrate enhanced results
            if enhanced_results:
                results.update(enhanced_results)
            
            # Run post-validation enhanced components
            if CI_COMPONENTS_AVAILABLE:
                validation_end_time = time.time()
                
                # Record core validation metrics
                if performance_tracker:
                    core_duration = validation_end_time - time.time()  # Approximate
                    performance_tracker.record_timing('core_validation', core_duration * 1000)
                    performance_tracker.record_metric('validation_success', 1 if results.get('success') else 0, 'boolean', 'result')
                    performance_tracker.record_metric('decision', results.get('decision', 'UNKNOWN'), 'string', 'result')
                    performance_tracker.record_metric('confidence', results.get('confidence', 0.0), 'ratio', 'result')
                    performance_tracker.record_metric('n_queries', results.get('n_queries', 0), 'count', 'result')
                
                # Run audit validation
                if getattr(args, 'enable_audit_validation', True):
                    logger.info("üõ°Ô∏è Running audit trail validation...")
                    audit_results = run_audit_validation(config.output_dir, logger)
                    results['audit_validation'] = audit_results
                    results['components_used'].append('audit_validation')
                    
                    if performance_tracker:
                        performance_tracker.record_metric('audit_entries_validated', 
                                                         audit_results.get('entries_validated', 0), 'count', 'audit')
                
                # Run adversarial attack simulation
                if getattr(args, 'enable_attack_simulation', False):
                    logger.info("‚öîÔ∏è Running adversarial attack simulation...")
                    attack_results = run_attack_simulation(args, logger)
                    results['attack_simulation'] = attack_results
                    results['components_used'].append('attack_simulation')
                    
                    if performance_tracker:
                        performance_tracker.record_metric('attacks_simulated', 
                                                         len(attack_results.get('attack_outcomes', [])), 'count', 'security')
                
                # Generate evidence bundle
                if getattr(args, 'generate_evidence_bundle', True):
                    logger.info("üì¶ Generating CI evidence bundle...")
                    evidence_path = generate_evidence_bundle(config.output_dir, results, logger)
                    if evidence_path:
                        results['evidence_bundle_path'] = evidence_path
                        results['evidence_paths'].append(evidence_path)
                        results['components_used'].append('evidence_generation')
                
                # Generate performance dashboard
                if getattr(args, 'performance_dashboard', False) and performance_tracker:
                    logger.info("üìä Generating performance dashboard...")
                    dashboard_path = generate_performance_dashboard(performance_tracker, config.output_dir, logger)
                    if dashboard_path:
                        results['dashboard_path'] = dashboard_path
                        results['components_used'].append('dashboard_generation')
                
                # Clean up performance tracking
                if performance_tracker:
                    performance_tracker.stop_monitoring()
                    performance_tracker.end_session()
                    
                    # Generate performance report
                    perf_report = performance_tracker.generate_performance_report(days=1)
                    results['performance_data'] = perf_report
            
            # Print enhanced summary
            if not args.quiet:
                if CI_COMPONENTS_AVAILABLE and results.get('components_used'):
                    print_enhanced_summary(results, logger)
                else:
                    print_summary(results, logger)
            
            # Generate HTML report if requested
            if args.generate_report and results.get('success'):
                logger.info("Generating HTML report...")
                generator = ReportGenerator(output_dir=Path(args.output_dir))
                
                # Load evidence bundle
                evidence_bundle = None
                if results.get('evidence_bundle_path'):
                    try:
                        with open(results['evidence_bundle_path'], 'r') as f:
                            evidence_bundle = json.load(f)
                    except Exception as e:
                        logger.warning(f"Could not load evidence bundle: {e}")
                
                # Generate report
                report_path = generator.generate_html_report(results, evidence_bundle)
                logger.info(f"HTML Report: {report_path}")
                
                # Also generate JSON summary
                summary_path = Path(args.output_dir) / f"summary_{results['run_id']}.json"
                generator.generate_summary_json(results, summary_path)
                logger.info(f"JSON Summary: {summary_path}")
        
        # Update README tables with latest results (if successful and enabled)
        if (results.get('success', False) and 
            not args.dry_run and 
            getattr(args, 'update_readme_tables', True)):
            try:
                from scripts.update_readme_tables import ReadmeTableUpdater
                
                logger.info("üìä Updating README tables with latest results...")
                updater = ReadmeTableUpdater()
                
                # Add this run to rolling metrics
                updater.add_recent_run_to_metrics(results)
                
                # Update README tables
                if updater.update_all_tables():
                    logger.info("‚úÖ README tables updated successfully")
                else:
                    logger.warning("‚ö†Ô∏è Some README table updates failed")
                    
            except ImportError:
                logger.debug("README table updater not available")
            except Exception as e:
                logger.warning(f"Failed to update README tables: {e}")
        
        # Exit with appropriate code
        if results.get('success', False):
            sys.exit(0)
        else:
            sys.exit(1)
    
    except KeyboardInterrupt:
        logger.warning("\nPipeline interrupted by user")
        sys.exit(130)
    
    except Exception as e:
        logger.error(f"Pipeline failed with error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()