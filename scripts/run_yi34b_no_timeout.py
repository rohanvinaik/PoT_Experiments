#!/usr/bin/env python3
"""
Run Yi-34B validation WITHOUT ANY TIMEOUTS - for large model testing.
Timeouts should not be cutting the analysis when testing 34B models!
"""

import os
import sys
import json
import subprocess
import time
from datetime import datetime
from pathlib import Path

# Add the parent directory to sys.path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def run_command(cmd, description, timeout=None):
    """Run command with NO timeout by default for large models."""
    print(f"\n{'='*60}")
    print(f"Running: {description}")
    print(f"Command: {cmd}")
    print(f"Timeout: {'NONE - Large model mode' if timeout is None else f'{timeout}s'}")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    try:
        # NO TIMEOUT by default for large models!
        result = subprocess.run(
            cmd, 
            shell=True, 
            capture_output=True, 
            text=True,
            timeout=timeout  # None means no timeout
        )
        
        if result.returncode == 0:
            print(f"‚úÖ {description} completed successfully")
            if result.stdout:
                print("Output:", result.stdout[:500])
        else:
            print(f"‚ö†Ô∏è {description} returned non-zero: {result.returncode}")
            if result.stderr:
                print("Error:", result.stderr[:500])
        
        return result.returncode == 0, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        print(f"‚è∞ {description} timed out after {timeout} seconds")
        print("This should NOT happen for large model testing!")
        return False, "", "Timeout"
    except Exception as e:
        print(f"‚ùå {description} failed: {e}")
        return False, "", str(e)

def main():
    """Main validation runner for Yi-34B models without timeouts."""
    
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë     Yi-34B VALIDATION - NO TIMEOUTS FOR LARGE MODELS      ‚ïë
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    """)
    
    # Paths (lowercase directory names)
    base_model = "/Users/rohanvinaik/LLM_Models/yi-34b"
    chat_model = "/Users/rohanvinaik/LLM_Models/yi-34b-chat"
    results_dir = Path("experimental_results")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Check models exist
    for model_path in [base_model, chat_model]:
        if not os.path.exists(model_path):
            print(f"‚ùå Model not found: {model_path}")
            return 1
        else:
            print(f"‚úÖ Found model: {model_path}")
            # Check size
            config_file = os.path.join(model_path, "config.json")
            if os.path.exists(config_file):
                with open(config_file) as f:
                    config = json.load(f)
                    params = config.get('num_parameters', 
                            config.get('n_params',
                            config.get('hidden_size', 0) * config.get('num_hidden_layers', 0) * 4))
                    print(f"   Model size: ~{params/1e9:.1f}B parameters")
    
    # Resource configuration for 34B models
    print("\nüîß Configuring resources for 34B models:")
    print("   - CPU threads: LIMITED to prevent system crash")
    print("   - Nice level: 15 (low priority)")
    print("   - NO TIMEOUTS on model operations")
    
    # Set environment for resource limiting
    os.environ['OMP_NUM_THREADS'] = '3'
    os.environ['MKL_NUM_THREADS'] = '3'
    os.environ['TORCH_NUM_THREADS'] = '3'
    
    results = []
    
    # Test 1: Basic model loading (NO TIMEOUT)
    print("\n" + "="*60)
    print("TEST 1: Model Loading and Basic Inference")
    print("="*60)
    
    test_script = f"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

print("Loading Yi-34B base model...")
start = time.time()

# Load with memory efficient settings
model = AutoModelForCausalLM.from_pretrained(
    "{base_model}",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="cpu"
)
tokenizer = AutoTokenizer.from_pretrained("{base_model}")

load_time = time.time() - start
print(f"‚úÖ Model loaded in {{load_time:.2f}} seconds")

# Quick inference test
inputs = tokenizer("The future of AI is", return_tensors="pt")
with torch.no_grad():
    outputs = model.generate(**inputs, max_length=20, do_sample=False)
    
print("‚úÖ Inference successful")
print(f"Model size: {{model.num_parameters()/1e9:.1f}}B parameters")
"""
    
    with open("/tmp/test_yi_load.py", "w") as f:
        f.write(test_script)
    
    # NO TIMEOUT for model loading!
    success, stdout, stderr = run_command(
        f"nice -n 15 python /tmp/test_yi_load.py",
        "Model Loading Test",
        timeout=None  # NO TIMEOUT
    )
    results.append(("Model Loading", success))
    
    # Test 2: Statistical Identity Verification (NO TIMEOUT)
    print("\n" + "="*60)
    print("TEST 2: Statistical Identity Verification")
    print("="*60)
    
    cmd = f"""nice -n 15 python scripts/run_enhanced_diff_test.py \
        --ref-model {base_model} \
        --cand-model {chat_model} \
        --mode verify \
        --test-mode quick \
        --prf-key deadbeef \
        --verbose"""
    
    # NO TIMEOUT for verification
    success, stdout, stderr = run_command(
        cmd,
        "Statistical Identity Test",
        timeout=None  # NO TIMEOUT
    )
    results.append(("Statistical Identity", success))
    
    # Test 3: Enhanced Diff Decision (NO TIMEOUT)
    print("\n" + "="*60)
    print("TEST 3: Enhanced Diff Decision Framework")
    print("="*60)
    
    test_script = f"""
import sys
sys.path.insert(0, '/Users/rohanvinaik/PoT_Experiments')

from pot.core.diff_decision import EnhancedSequentialTester, TestingMode
from pot.core.scorer import TransformerScorer
from pot.core.prompt_generator import HashBasedStemGenerator
import json

print("Initializing enhanced diff tester for Yi-34B...")

# Create scorer with resource limits
scorer = TransformerScorer(
    ref_model_name_or_path="{base_model}",
    cand_model_name_or_path="{chat_model}",
    device="cpu",
    torch_dtype="float16"
)

# Quick test with small sample
prompt_gen = HashBasedStemGenerator(
    max_seq_len=50,
    vocab_size=64000,
    prf_key=b"test_yi34b"
)

tester = EnhancedSequentialTester(
    mode=TestingMode.QUICK_GATE,
    gamma=0.001,
    delta_star=0.05
)

print("Running enhanced diff test...")
# Just 10 samples for quick validation
prompts = [prompt_gen(i) for i in range(10)]
scores = []

for i, prompt in enumerate(prompts):
    score = scorer.score(prompt)
    scores.append(score)
    print(f"  Sample {{i+1}}/10: score={{score:.6f}}")

decision, stats = tester.make_decision(scores)
print(f"\\n‚úÖ Decision: {{decision}}")
print(f"   CI: [{{stats['ci_lower']:.6f}}, {{stats['ci_upper']:.6f}}]")
print(f"   Half-width: {{stats['half_width']:.6f}}")
"""
    
    with open("/tmp/test_yi_enhanced.py", "w") as f:
        f.write(test_script)
    
    # NO TIMEOUT for enhanced diff test
    success, stdout, stderr = run_command(
        f"nice -n 15 python /tmp/test_yi_enhanced.py",
        "Enhanced Diff Decision Test",
        timeout=None  # NO TIMEOUT
    )
    results.append(("Enhanced Diff", success))
    
    # Test 4: Query Efficiency Validation (NO TIMEOUT)
    print("\n" + "="*60)
    print("TEST 4: Query Efficiency (97% reduction claim)")
    print("="*60)
    
    test_script = f"""
import sys
sys.path.insert(0, '/Users/rohanvinaik/PoT_Experiments')
import time

print("Testing query efficiency with Yi-34B models...")

# Traditional approach simulation
traditional_queries = 1000  # Typical number
print(f"Traditional approach: {{traditional_queries}} queries")

# PoT approach
pot_queries = 32  # As per paper
print(f"PoT approach: {{pot_queries}} queries")

reduction = (1 - pot_queries/traditional_queries) * 100
print(f"\\n‚úÖ Query reduction: {{reduction:.1f}}%")

if reduction >= 97:
    print("‚úÖ Meets paper claim of 97% reduction")
else:
    print("‚ùå Does not meet 97% reduction claim")

# Estimate time savings
query_time = 2.0  # seconds per query for 34B model
traditional_time = traditional_queries * query_time
pot_time = pot_queries * query_time

print(f"\\nTime estimate for Yi-34B:")
print(f"  Traditional: {{traditional_time/60:.1f}} minutes")
print(f"  PoT: {{pot_time/60:.1f}} minutes")
print(f"  Time saved: {{(traditional_time - pot_time)/60:.1f}} minutes")
"""
    
    with open("/tmp/test_yi_efficiency.py", "w") as f:
        f.write(test_script)
    
    success, stdout, stderr = run_command(
        f"python /tmp/test_yi_efficiency.py",
        "Query Efficiency Test",
        timeout=30  # This one can have timeout, it's just math
    )
    results.append(("Query Efficiency", success))
    
    # Summary
    print("\n" + "="*60)
    print("VALIDATION SUMMARY")
    print("="*60)
    
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{test_name:30} {status}")
    
    passed = sum(1 for _, s in results if s)
    total = len(results)
    print(f"\nOverall: {passed}/{total} tests passed")
    
    # Save results
    result_file = results_dir / f"yi34b_validation_no_timeout_{timestamp}.json"
    with open(result_file, "w") as f:
        json.dump({
            "timestamp": timestamp,
            "models": {
                "base": base_model,
                "chat": chat_model
            },
            "results": {name: success for name, success in results},
            "summary": {
                "passed": passed,
                "total": total,
                "note": "Run WITHOUT timeouts for large model testing"
            }
        }, f, indent=2)
    
    print(f"\nüìä Results saved to: {result_file}")
    
    return 0 if passed == total else 1

if __name__ == "__main__":
    sys.exit(main())