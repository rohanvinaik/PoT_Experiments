#!/usr/bin/env bash
set -euo pipefail

# Zero-Knowledge Proof System Test Runner
# Comprehensive testing of ZK prover binaries and integration

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
PYTHON=${PYTHON:-python3}
RESULTS_DIR="experimental_results/zk_tests"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
ZK_METRICS_FILE="${RESULTS_DIR}/zk_metrics_${TIMESTAMP}.json"

# Ensure repository root on PYTHONPATH
export PYTHONPATH="${PYTHONPATH:-$(pwd)}"

# Print colored output
print_header() {
    echo -e "\n${BLUE}========================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}========================================${NC}\n"
}

print_success() {
    echo -e "${GREEN}‚úì $1${NC}"
}

print_error() {
    echo -e "${RED}‚úó $1${NC}"
}

print_info() {
    echo -e "${YELLOW}‚Ñπ $1${NC}"
}

# Create results directory
mkdir -p "${RESULTS_DIR}"

print_header "ZERO-KNOWLEDGE PROOF SYSTEM TEST SUITE"
echo "Started at: $(date)"
echo "Python: ${PYTHON}"
echo "Results directory: ${RESULTS_DIR}"
echo "Metrics file: ${ZK_METRICS_FILE}"

# Test tracking
TOTAL_TESTS=0
PASSED_TESTS=0
FAILED_TESTS=0

# Test 1: ZK Integration Tests
print_header "RUNNING ZK INTEGRATION TESTS"
print_info "Testing ZK prover integration with PoT framework"

if PYTHONPATH="${PWD}:${PYTHONPATH:-}" ${PYTHON} -m pytest tests/test_zk_integration.py -v > "${RESULTS_DIR}/zk_integration_${TIMESTAMP}.log" 2>&1; then
    print_success "ZK integration tests passed"
    PASSED_TESTS=$((PASSED_TESTS + 1))
else
    print_error "ZK integration tests failed"
    FAILED_TESTS=$((FAILED_TESTS + 1))
    print_info "Check ${RESULTS_DIR}/zk_integration_${TIMESTAMP}.log for details"
fi
TOTAL_TESTS=$((TOTAL_TESTS + 1))

# Test 2: ZK Error Handling Tests with Metrics
print_header "RUNNING ZK ERROR HANDLING TESTS"
print_info "Testing comprehensive error handling and retry logic with metrics collection"

if ZK_METRICS_FILE="${ZK_METRICS_FILE}" PYTHONPATH="${PWD}:${PYTHONPATH:-}" ${PYTHON} scripts/test_improved_error_handling.py > "${RESULTS_DIR}/zk_error_handling_${TIMESTAMP}.log" 2>&1; then
    print_success "ZK error handling tests passed"
    PASSED_TESTS=$((PASSED_TESTS + 1))
else
    print_error "ZK error handling tests failed"
    FAILED_TESTS=$((FAILED_TESTS + 1))
    print_info "Check ${RESULTS_DIR}/zk_error_handling_${TIMESTAMP}.log for details"
fi
TOTAL_TESTS=$((TOTAL_TESTS + 1))

# Test 3: ZK Validation Suite
print_header "RUNNING ZK VALIDATION SUITE"
print_info "Testing circuit validation and proof verification"

if PYTHONPATH="${PWD}:${PYTHONPATH:-}" ${PYTHON} -m pytest tests/test_zk_validation_suite.py -v > "${RESULTS_DIR}/zk_validation_${TIMESTAMP}.log" 2>&1; then
    print_success "ZK validation suite passed"
    PASSED_TESTS=$((PASSED_TESTS + 1))
else
    print_error "ZK validation suite failed"
    FAILED_TESTS=$((FAILED_TESTS + 1))
    print_info "Check ${RESULTS_DIR}/zk_validation_${TIMESTAMP}.log for details"
fi
TOTAL_TESTS=$((TOTAL_TESTS + 1))

# Test 4: Error Handling Demo
print_header "RUNNING ERROR HANDLING DEMONSTRATION"
print_info "Demonstrating improved error handling features"

if PYTHONPATH="${PWD}:${PYTHONPATH:-}" ${PYTHON} scripts/demo_error_handling_improvements.py > "${RESULTS_DIR}/error_demo_${TIMESTAMP}.log" 2>&1; then
    print_success "Error handling demonstration completed"
    PASSED_TESTS=$((PASSED_TESTS + 1))
else
    print_error "Error handling demonstration failed"
    FAILED_TESTS=$((FAILED_TESTS + 1))
    print_info "Check ${RESULTS_DIR}/error_demo_${TIMESTAMP}.log for details"
fi
TOTAL_TESTS=$((TOTAL_TESTS + 1))

# Test 5: ZK Binary Functionality Tests
print_header "RUNNING ZK BINARY FUNCTIONALITY TESTS"
print_info "Testing Rust prover binaries with real inputs"

ZK_BINARY_TESTS_PASSED=0
ZK_BINARY_TESTS_TOTAL=0

# Test SGD prover binary
prover_dir="pot/zk/prover_halo2"
sgd_binary="$prover_dir/target/release/prove_sgd_stdin"

if [ -f "$sgd_binary" ] && [ -x "$sgd_binary" ]; then
    print_info "Testing SGD prover binary..."
    
    # Create test input
    test_input='{"weights_before": [1.0, 2.0], "weights_after": [0.9, 1.8], "gradients": [0.1, 0.2], "learning_rate": 0.01}'
    
    if timeout 10 echo "$test_input" | "$sgd_binary" > "${RESULTS_DIR}/sgd_binary_test_${TIMESTAMP}.log" 2>&1; then
        print_success "SGD binary test passed"
        ZK_BINARY_TESTS_PASSED=$((ZK_BINARY_TESTS_PASSED + 1))
    else
        print_error "SGD binary test failed"
        print_info "Check ${RESULTS_DIR}/sgd_binary_test_${TIMESTAMP}.log for details"
    fi
else
    print_error "SGD binary not found or not executable: $sgd_binary"
fi
ZK_BINARY_TESTS_TOTAL=$((ZK_BINARY_TESTS_TOTAL + 1))

# Test LoRA prover binary
lora_binary="$prover_dir/target/release/prove_lora_stdin"

if [ -f "$lora_binary" ] && [ -x "$lora_binary" ]; then
    print_info "Testing LoRA prover binary..."
    
    # Create test input for LoRA
    lora_test_input='{"adapter_a": [[1.0, 2.0], [3.0, 4.0]], "adapter_b": [[0.5, 1.0], [1.5, 2.0]]}'
    
    if timeout 10 echo "$lora_test_input" | "$lora_binary" > "${RESULTS_DIR}/lora_binary_test_${TIMESTAMP}.log" 2>&1; then
        print_success "LoRA binary test passed"
        ZK_BINARY_TESTS_PASSED=$((ZK_BINARY_TESTS_PASSED + 1))
    else
        print_error "LoRA binary test failed"
        print_info "Check ${RESULTS_DIR}/lora_binary_test_${TIMESTAMP}.log for details"
    fi
else
    print_error "LoRA binary not found or not executable: $lora_binary"
fi
ZK_BINARY_TESTS_TOTAL=$((ZK_BINARY_TESTS_TOTAL + 1))

# Update test counts
if [ "$ZK_BINARY_TESTS_PASSED" -eq "$ZK_BINARY_TESTS_TOTAL" ]; then
    PASSED_TESTS=$((PASSED_TESTS + 1))
    print_success "All ZK binary tests passed ($ZK_BINARY_TESTS_PASSED/$ZK_BINARY_TESTS_TOTAL)"
else
    FAILED_TESTS=$((FAILED_TESTS + 1))
    print_error "Some ZK binary tests failed ($ZK_BINARY_TESTS_PASSED/$ZK_BINARY_TESTS_TOTAL)"
fi
TOTAL_TESTS=$((TOTAL_TESTS + 1))

# Collect final metrics and generate summary
print_header "COLLECTING ZK METRICS"
print_info "Generating comprehensive metrics report..."

# Save metrics to file
if ${PYTHON} -c "
import sys
sys.path.append('.')
from pot.zk.metrics import get_zk_metrics_collector
collector = get_zk_metrics_collector()
collector.save_report('${ZK_METRICS_FILE}')
print('‚úÖ Metrics saved to ${ZK_METRICS_FILE}')
" 2>/dev/null; then
    print_success "ZK metrics collected successfully"
else
    print_info "No metrics to collect (tests may not have used metrics system)"
fi

# Generate ZK test summary
print_header "ZK TEST SUMMARY"

cat > "${RESULTS_DIR}/zk_test_summary_${TIMESTAMP}.txt" << EOF
ZERO-KNOWLEDGE PROOF SYSTEM TEST RESULTS
========================================

Test Date: $(date)
Python Version: $(${PYTHON} --version 2>&1)
Rust Version: $(rustc --version 2>/dev/null || echo "Not available")

Test Results:
- Total Tests: $TOTAL_TESTS
- Passed: $PASSED_TESTS
- Failed: $FAILED_TESTS
- Success Rate: $(python3 -c "print(f'{($PASSED_TESTS/$TOTAL_TESTS)*100:.1f}%')")

Individual Test Results:
$([ $PASSED_TESTS -gt 0 ] && echo "‚úÖ ZK Integration Tests" || echo "‚ùå ZK Integration Tests")
$([ $PASSED_TESTS -gt 1 ] && echo "‚úÖ Error Handling Tests" || echo "‚ùå Error Handling Tests") 
$([ $PASSED_TESTS -gt 2 ] && echo "‚úÖ ZK Validation Suite" || echo "‚ùå ZK Validation Suite")
$([ $PASSED_TESTS -gt 3 ] && echo "‚úÖ Error Handling Demo" || echo "‚ùå Error Handling Demo")
$([ $PASSED_TESTS -gt 4 ] && echo "‚úÖ Binary Functionality Tests ($ZK_BINARY_TESTS_PASSED/$ZK_BINARY_TESTS_TOTAL)" || echo "‚ùå Binary Functionality Tests ($ZK_BINARY_TESTS_PASSED/$ZK_BINARY_TESTS_TOTAL)")

Key Features Tested:
‚úÖ Exception hierarchy with detailed error information
‚úÖ Automatic model type detection (SGD vs LoRA)  
‚úÖ Retry logic with exponential backoff
‚úÖ Configurable failure handling modes
‚úÖ Training auditor integration
‚úÖ Halo2 circuit constraint verification
‚úÖ Poseidon hash integration
‚úÖ Proof aggregation and caching
‚úÖ Python-Rust binary integration

Status: $([ $PASSED_TESTS -eq $TOTAL_TESTS ] && echo "‚úÖ ALL TESTS PASSED - ZK SYSTEM READY FOR PRODUCTION" || echo "‚ö†Ô∏è SOME TESTS FAILED - REVIEW LOGS FOR DETAILS")

Log Files:
- ZK Integration: ${RESULTS_DIR}/zk_integration_${TIMESTAMP}.log
- Error Handling: ${RESULTS_DIR}/zk_error_handling_${TIMESTAMP}.log
- Validation Suite: ${RESULTS_DIR}/zk_validation_${TIMESTAMP}.log
- Error Demo: ${RESULTS_DIR}/error_demo_${TIMESTAMP}.log
- SGD Binary: ${RESULTS_DIR}/sgd_binary_test_${TIMESTAMP}.log
- LoRA Binary: ${RESULTS_DIR}/lora_binary_test_${TIMESTAMP}.log

EOF

print_success "ZK test summary generated: ${RESULTS_DIR}/zk_test_summary_${TIMESTAMP}.txt"

# Display summary
cat "${RESULTS_DIR}/zk_test_summary_${TIMESTAMP}.txt"

# Set exit code
if [ "$PASSED_TESTS" -eq "$TOTAL_TESTS" ]; then
    print_success "üéâ ALL ZK TESTS PASSED - SYSTEM READY FOR PRODUCTION"
    exit 0
else
    print_error "‚ùå $FAILED_TESTS/$TOTAL_TESTS ZK tests failed"
    print_info "Review log files in ${RESULTS_DIR}/ for detailed error information"
    exit 1
fi